"""
Script fine-tuning PhoWhisper v·ªõi d·ªØ li·ªáu JSONL

S·ª≠ d·ª•ng th∆∞ vi·ªán transformers v√† datasets t·ª´ Hugging Face
Ch·ªâ h·ªó tr·ª£ PhoWhisper models (vinai/PhoWhisper-base, vinai/PhoWhisper-large)
"""

import os
import json
import argparse
from pathlib import Path
import torch
import numpy as np
from datasets import load_dataset, Audio
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)
from transformers.models.whisper.english_normalizer import BasicTextNormalizer
from transformers import TrainerCallback


def load_jsonl_dataset(jsonl_file, audio_dir, use_negative_samples=False):
    """
    Load dataset t·ª´ JSONL file
    H·ªó tr·ª£ dataset c√≥ c·∫£ positive (is_match=True) v√† negative (is_match=False) samples
    
    Args:
        jsonl_file: File JSONL ch·ª©a {"audio": "path", "sentence": "text", "is_match": bool}
        audio_dir: Th∆∞ m·ª•c ch·ª©a audio files (base directory, v√≠ d·ª•: archive/vivos/train/waves)
        use_negative_samples: N·∫øu True, s·∫Ω train c·∫£ negative samples. M·∫∑c ƒë·ªãnh False (ch·ªâ train positive)
    
    Returns:
        Dataset v·ªõi audio paths ƒë√£ ƒë∆∞·ª£c resolve th√†nh absolute paths
    """
    # Validate inputs
    if not os.path.exists(jsonl_file):
        raise FileNotFoundError(f"JSONL file kh√¥ng t·ªìn t·∫°i: {jsonl_file}")
    
    if not os.path.exists(audio_dir):
        raise FileNotFoundError(f"Audio directory kh√¥ng t·ªìn t·∫°i: {audio_dir}")
    
    print(f"ƒêang load dataset t·ª´: {jsonl_file}")
    print(f"Audio directory: {audio_dir}")
    
    # ƒê·ªçc JSONL file
    data = []
    with open(jsonl_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    print(f"Warning: Kh√¥ng th·ªÉ parse d√≤ng JSON: {line[:50]}... Error: {e}")
    
    print(f"ƒê√£ ƒë·ªçc {len(data)} entries t·ª´ JSONL")
    
    # Convert audio_dir to absolute path
    audio_dir = os.path.abspath(audio_dir)
    
    # Ph√¢n lo·∫°i v√† ki·ªÉm tra entries
    positive_data = []
    negative_data = []
    missing_files = []
    
    for idx, item in enumerate(data):
        if 'audio' not in item or 'sentence' not in item:
            print(f"Warning: Entry {idx} thi·∫øu 'audio' ho·∫∑c 'sentence': {item}")
            continue
        
        # Ki·ªÉm tra is_match (m·∫∑c ƒë·ªãnh True n·∫øu kh√¥ng c√≥ field n√†y - t∆∞∆°ng th√≠ch ng∆∞·ª£c)
        is_match = item.get('is_match', True)
        
        audio_path = item['audio']
        # Normalize path: thay backslash b·∫±ng forward slash (cross-platform)
        audio_path = audio_path.replace('\\', '/')
        
        # N·∫øu l√† relative path, th√™m audio_dir v√†o
        if not os.path.isabs(audio_path):
            # Join v·ªõi audio_dir v√† normalize path
            audio_path = os.path.join(audio_dir, audio_path)
            audio_path = os.path.normpath(audio_path)
        else:
            audio_path = os.path.normpath(audio_path)
        
        # Ki·ªÉm tra file c√≥ t·ªìn t·∫°i kh√¥ng
        if os.path.exists(audio_path):
            # L∆∞u absolute path
            item['audio'] = os.path.abspath(audio_path)
            if is_match:
                positive_data.append(item)
            else:
                negative_data.append(item)
        else:
            missing_files.append(audio_path)
            if len(missing_files) <= 10:  # Ch·ªâ hi·ªÉn th·ªã 10 file ƒë·∫ßu ti√™n
                print(f"Warning: File kh√¥ng t·ªìn t·∫°i: {audio_path}")
    
    if missing_files:
        print(f"Warning: T·ªïng c·ªông {len(missing_files)} files kh√¥ng t·ªìn t·∫°i (ƒë√£ hi·ªÉn th·ªã 10 ƒë·∫ßu ti√™n)")
    
    # Th·ªëng k√™
    print(f"\nPh√¢n lo·∫°i entries:")
    print(f"  - Positive (is_match=True): {len(positive_data)}")
    print(f"  - Negative (is_match=False): {len(negative_data)}")
    
    # Ch·ªçn data ƒë·ªÉ train
    if use_negative_samples:
        valid_data = positive_data + negative_data
        print(f"  - S·∫Ω train tr√™n c·∫£ positive v√† negative: {len(valid_data)} samples")
    else:
        valid_data = positive_data
        print(f"  - Ch·ªâ train tr√™n positive samples: {len(valid_data)} samples")
        if len(negative_data) > 0:
            print(f"  - B·ªè qua {len(negative_data)} negative samples (d√πng --use-negative-samples ƒë·ªÉ train c·∫£ negative)")
    
    if len(valid_data) == 0:
        raise ValueError(f"Kh√¥ng c√≥ entries h·ª£p l·ªá n√†o ƒë·ªÉ train! Ki·ªÉm tra l·∫°i paths trong JSONL v√† audio_dir.")
    
    # T·∫°o dataset t·ª´ JSONL file v·ªõi absolute paths
    # KH√îNG load audio v√†o memory - ch·ªâ l∆∞u paths ƒë·ªÉ x·ª≠ l√Ω on-the-fly
    from datasets import Dataset
    from pathlib import Path
    
    # T·∫°o dataset tr·ª±c ti·∫øp t·ª´ list of dicts
    # Gi·ªØ audio paths d∆∞·ªõi d·∫°ng strings (kh√¥ng load audio v√†o memory)
    print("ƒêang t·∫°o dataset t·ª´ data (ch·ªâ l∆∞u paths, kh√¥ng load audio)...")
    
    # Chu·∫©n b·ªã data v·ªõi audio paths d∆∞·ªõi d·∫°ng strings
    dataset_data = []
    for item in valid_data:
        # ƒê·∫£m b·∫£o audio path l√† absolute path string
        audio_path = str(Path(item["audio"]).resolve())
        dataset_data.append({
            "audio": audio_path,  # L∆∞u path string, kh√¥ng load audio
            "sentence": item["sentence"]
        })
    
    # T·∫°o Dataset object t·ª´ list of dicts
    # Audio column l√† strings (paths), kh√¥ng ph·∫£i Audio objects
    dataset = Dataset.from_list(dataset_data)
    
    print("Dataset ƒë√£ s·∫µn s√†ng (audio s·∫Ω ƒë∆∞·ª£c load on-the-fly trong training)")
    
    return dataset


def prepare_dataset(batch, processor):
    """Chu·∫©n b·ªã d·ªØ li·ªáu cho training"""
    # Load v√† resample audio (batch processing)
    audio_arrays = []
    sampling_rates = []
    sentences = []
    
    # X·ª≠ l√Ω t·ª´ng item trong batch
    for item in batch["audio"]:
        audio_arrays.append(item["array"])
        sampling_rates.append(item["sampling_rate"])
    
    for sentence in batch["sentence"]:
        sentences.append(sentence)
    
    # Compute log-Mel input features t·ª´ audio arrays
    inputs = processor.feature_extractor(
        audio_arrays, 
        sampling_rate=sampling_rates[0] if sampling_rates else 16000,
        return_tensors="np"
    ).input_features
    
    # Encode target text th√†nh label ids
    labels = processor.tokenizer(
        sentences,
        return_tensors="np",
        padding=True,
        truncation=True
    ).input_ids
    
    # Replace padding token id's c·ªßa the labels b·∫±ng -100 ƒë·ªÉ ignore trong loss
    labels = [
        [(label if label != processor.tokenizer.pad_token_id else -100) for label in label_ids]
        for label_ids in labels
    ]
    
    return {
        "input_features": inputs.tolist(),
        "labels": labels
    }


def compute_wer(reference, hypothesis):
    """T√≠nh Word Error Rate (WER) s·ª≠ d·ª•ng dynamic programming"""
    ref_words = reference.strip().lower().split()
    hyp_words = hypothesis.strip().lower().split()
    
    if len(ref_words) == 0:
        return 1.0 if len(hyp_words) > 0 else 0.0
    
    # Dynamic programming ƒë·ªÉ t√≠nh edit distance
    n, m = len(ref_words), len(hyp_words)
    dp = np.zeros((n + 1, m + 1), dtype=int)
    
    # Initialize
    for i in range(n + 1):
        dp[i][0] = i  # deletions
    for j in range(m + 1):
        dp[0][j] = j  # insertions
    
    # Fill DP table
    for i in range(1, n + 1):
        for j in range(1, m + 1):
            if ref_words[i-1] == hyp_words[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = min(
                    dp[i-1][j] + 1,      # deletion
                    dp[i][j-1] + 1,      # insertion
                    dp[i-1][j-1] + 1     # substitution
                )
    
    return dp[n][m] / n


def compute_cer(reference, hypothesis):
    """T√≠nh Character Error Rate (CER)"""
    ref_chars = list(reference.strip().lower().replace(' ', ''))
    hyp_chars = list(hypothesis.strip().lower().replace(' ', ''))
    
    if len(ref_chars) == 0:
        return 1.0 if len(hyp_chars) > 0 else 0.0
    
    n, m = len(ref_chars), len(hyp_chars)
    dp = np.zeros((n + 1, m + 1), dtype=int)
    
    for i in range(n + 1):
        dp[i][0] = i
    for j in range(m + 1):
        dp[0][j] = j
    
    for i in range(1, n + 1):
        for j in range(1, m + 1):
            if ref_chars[i-1] == hyp_chars[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + 1)
    
    return dp[n][m] / n


def compute_metrics(pred, processor):
    """T√≠nh to√°n c√°c metrics: WER, CER, v√† c√°c ch·ªâ s·ªë kh√°c"""
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    
    # Decode predictions
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)
    
    # Normalize text
    normalizer = BasicTextNormalizer()
    pred_str = [normalizer(pred) for pred in pred_str]
    label_str = [normalizer(label) for label in label_str]
    
    # T√≠nh c√°c metrics
    wer_scores = []
    cer_scores = []
    exact_matches = 0
    total_samples = len(pred_str)
    
    for pred, label in zip(pred_str, label_str):
        wer = compute_wer(label, pred)
        cer = compute_cer(label, pred)
        wer_scores.append(wer)
        cer_scores.append(cer)
        
        # Exact match (case-insensitive)
        if pred.strip().lower() == label.strip().lower():
            exact_matches += 1
    
    # T√≠nh trung b√¨nh
    avg_wer = np.mean(wer_scores) if wer_scores else 1.0
    avg_cer = np.mean(cer_scores) if cer_scores else 1.0
    accuracy = exact_matches / total_samples if total_samples > 0 else 0.0
    
    # Word-level accuracy
    total_words = 0
    correct_words = 0
    for pred, label in zip(pred_str, label_str):
        pred_words = pred.strip().lower().split()
        label_words = label.strip().lower().split()
        total_words += len(label_words)
        min_len = min(len(pred_words), len(label_words))
        correct_words += sum(1 for i in range(min_len) if pred_words[i] == label_words[i])
    
    word_accuracy = correct_words / total_words if total_words > 0 else 0.0
    
    return {
        "wer": avg_wer,
        "cer": avg_cer,
        "accuracy": accuracy,
        "word_accuracy": word_accuracy,
        "exact_matches": exact_matches,
        "total_samples": total_samples
    }


def format_metrics_table(metrics_dict, dataset_name="Evaluation"):
    """T·∫°o b·∫£ng metrics ƒë·∫πp ƒë·ªÉ hi·ªÉn th·ªã"""
    lines = []
    lines.append("=" * 70)
    lines.append(f"üìä B·∫¢NG K·∫æT QU·∫¢ ƒê√ÅNH GI√Å: {dataset_name}")
    lines.append("=" * 70)
    
    # ƒê·ªãnh nghƒ©a c√°c metrics v√† m√¥ t·∫£
    metric_info = [
        ("WER (Word Error Rate)", "wer", "T·ª∑ l·ªá l·ªói t·ª´ (c√†ng th·∫•p c√†ng t·ªët, 0.0 = ho√†n h·∫£o)"),
        ("CER (Character Error Rate)", "cer", "T·ª∑ l·ªá l·ªói k√Ω t·ª± (c√†ng th·∫•p c√†ng t·ªët, 0.0 = ho√†n h·∫£o)"),
        ("Accuracy (Exact Match)", "accuracy", "T·ª∑ l·ªá c√¢u ch√≠nh x√°c ho√†n to√†n (c√†ng cao c√†ng t·ªët, 1.0 = ho√†n h·∫£o)"),
        ("Word Accuracy", "word_accuracy", "T·ª∑ l·ªá t·ª´ ch√≠nh x√°c (c√†ng cao c√†ng t·ªët, 1.0 = ho√†n h·∫£o)"),
    ]
    
    # Hi·ªÉn th·ªã t·ª´ng metric
    for metric_name, metric_key, description in metric_info:
        if metric_key in metrics_dict:
            value = metrics_dict[metric_key]
            if isinstance(value, float):
                if metric_key in ["wer", "cer"]:
                    # WER v√† CER: hi·ªÉn th·ªã d∆∞·ªõi d·∫°ng ph·∫ßn trƒÉm v√† s·ªë th·∫≠p ph√¢n
                    lines.append(f"\n{metric_name}:")
                    lines.append(f"  Gi√° tr·ªã: {value:.4f} ({value*100:.2f}%)")
                    lines.append(f"  M√¥ t·∫£: {description}")
                    # ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng
                    if value < 0.1:
                        quality = "Xu·∫•t s·∫Øc ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"
                    elif value < 0.2:
                        quality = "R·∫•t t·ªët ‚≠ê‚≠ê‚≠ê‚≠ê"
                    elif value < 0.3:
                        quality = "T·ªët ‚≠ê‚≠ê‚≠ê"
                    elif value < 0.5:
                        quality = "Kh√° ‚≠ê‚≠ê"
                    else:
                        quality = "C·∫ßn c·∫£i thi·ªán ‚≠ê"
                    lines.append(f"  ƒê√°nh gi√°: {quality}")
                else:
                    # Accuracy: hi·ªÉn th·ªã d∆∞·ªõi d·∫°ng ph·∫ßn trƒÉm
                    lines.append(f"\n{metric_name}:")
                    lines.append(f"  Gi√° tr·ªã: {value:.4f} ({value*100:.2f}%)")
                    lines.append(f"  M√¥ t·∫£: {description}")
                    # ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng
                    if value > 0.9:
                        quality = "Xu·∫•t s·∫Øc ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"
                    elif value > 0.8:
                        quality = "R·∫•t t·ªët ‚≠ê‚≠ê‚≠ê‚≠ê"
                    elif value > 0.7:
                        quality = "T·ªët ‚≠ê‚≠ê‚≠ê"
                    elif value > 0.5:
                        quality = "Kh√° ‚≠ê‚≠ê"
                    else:
                        quality = "C·∫ßn c·∫£i thi·ªán ‚≠ê"
                    lines.append(f"  ƒê√°nh gi√°: {quality}")
    
    # Th√¥ng tin b·ªï sung
    if "exact_matches" in metrics_dict and "total_samples" in metrics_dict:
        exact = metrics_dict["exact_matches"]
        total = metrics_dict["total_samples"]
        lines.append(f"\nüìà Th·ªëng k√™:")
        lines.append(f"  S·ªë m·∫´u ƒë√°nh gi√°: {total}")
        lines.append(f"  S·ªë c√¢u ch√≠nh x√°c ho√†n to√†n: {exact}")
        lines.append(f"  S·ªë c√¢u c√≥ l·ªói: {total - exact}")
    
    lines.append("=" * 70)
    return "\n".join(lines)


def save_detailed_report(results, output_dir, model_name):
    """L∆∞u b√°o c√°o chi ti·∫øt ra file text v√† JSON"""
    import datetime
    
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # T·∫°o b√°o c√°o text
    report_lines = []
    report_lines.append("=" * 70)
    report_lines.append("üìã B√ÅO C√ÅO K·∫æT QU·∫¢ HU·∫§N LUY·ªÜN M√î H√åNH")
    report_lines.append("=" * 70)
    report_lines.append(f"Model: {model_name}")
    report_lines.append(f"Th·ªùi gian: {timestamp}")
    report_lines.append(f"Th∆∞ m·ª•c output: {output_dir}")
    report_lines.append("")
    
    # Th√™m t·ª´ng ph·∫ßn ƒë√°nh gi√°
    if "eval_during_training" in results:
        report_lines.append(format_metrics_table(
            results["eval_during_training"], 
            "Validation Set (Trong qu√° tr√¨nh training)"
        ))
        report_lines.append("")
    
    if "eval_after_training" in results:
        eval_info = results["eval_after_training"]
        dataset_path = eval_info.get("path", "Unknown")
        metrics = eval_info.get("metrics", {})
        report_lines.append(format_metrics_table(
            metrics,
            f"Test Set (Sau khi training) - {os.path.basename(dataset_path)}"
        ))
        report_lines.append("")
    
    # T·ªïng k·∫øt
    report_lines.append("=" * 70)
    report_lines.append("üìå T·ªîNG K·∫æT")
    report_lines.append("=" * 70)
    
    best_wer = float('inf')
    best_dataset = None
    
    if "eval_during_training" in results:
        wer = results["eval_during_training"].get("wer", float('inf'))
        if wer < best_wer:
            best_wer = wer
            best_dataset = "Validation Set"
    
    if "eval_after_training" in results:
        wer = results["eval_after_training"].get("metrics", {}).get("wer", float('inf'))
        if wer < best_wer:
            best_wer = wer
            best_dataset = "Test Set"
    
    if best_dataset:
        report_lines.append(f"WER t·ªët nh·∫•t: {best_wer:.4f} ({best_wer*100:.2f}%) tr√™n {best_dataset}")
        if best_wer < 0.1:
            report_lines.append("üéâ M√¥ h√¨nh ƒë·∫°t ch·∫•t l∆∞·ª£ng xu·∫•t s·∫Øc!")
        elif best_wer < 0.2:
            report_lines.append("‚úÖ M√¥ h√¨nh ƒë·∫°t ch·∫•t l∆∞·ª£ng r·∫•t t·ªët!")
        elif best_wer < 0.3:
            report_lines.append("üëç M√¥ h√¨nh ƒë·∫°t ch·∫•t l∆∞·ª£ng t·ªët!")
        else:
            report_lines.append("‚ö†Ô∏è  M√¥ h√¨nh c·∫ßn ƒë∆∞·ª£c c·∫£i thi·ªán th√™m.")
    
    report_lines.append("=" * 70)
    
    # L∆∞u file text
    text_report_path = os.path.join(output_dir, "evaluation_report.txt")
    try:
        with open(text_report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report_lines))
        print(f"‚úÖ ƒê√£ l∆∞u b√°o c√°o text: {text_report_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ l∆∞u b√°o c√°o text: {e}")
    
    # L∆∞u file JSON (ƒë√£ c√≥ s·∫µn trong code ch√≠nh)
    json_report_path = os.path.join(output_dir, "training_report.json")
    try:
        # Th√™m metadata v√†o JSON
        json_results = {
            "model_name": model_name,
            "timestamp": timestamp,
            "output_dir": output_dir,
            "results": results
        }
        with open(json_report_path, "w", encoding="utf-8") as f:
            json.dump(json_results, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ ƒê√£ l∆∞u b√°o c√°o JSON: {json_report_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ l∆∞u b√°o c√°o JSON: {e}")
    
    # In b√°o c√°o ra console
    print("\n" + "\n".join(report_lines))


class WhisperDataCollator:
    """Custom data collator x·ª≠ l√Ω audio on-the-fly v√† t·ªëi ∆∞u VRAM"""
    
    def __init__(self, processor, tokenizer, padding=True, device="cuda", enable_cache=False, enable_augmentation=False):
        self.processor = processor
        self.tokenizer = tokenizer
        self.padding = padding
        self.device = device if torch.cuda.is_available() else "cpu"
        # T·∫Øt cache ƒë·ªÉ ti·∫øt ki·ªám VRAM (cache audio t·ªën nhi·ªÅu RAM)
        self.enable_cache = enable_cache
        self.enable_augmentation = enable_augmentation  # B·∫≠t data augmentation khi training
        self._audio_cache = {} if enable_cache else {}  # Cache ch·ªâ d√πng khi enable_cache=True
        self._cache_size_limit = 0  # T·∫Øt cache ƒë·ªÉ ti·∫øt ki·ªám VRAM
    
    def _load_audio(self, audio_path, max_duration=30.0, enable_augmentation=False):
        """
        Load audio t·ª´ file v·ªõi error handling - t·ªëi ∆∞u RAM
        Ch·ªâ load t·ªëi ƒëa max_duration gi√¢y (m·∫∑c ƒë·ªãnh 30s = 3000 frames mel spectrogram)
        
        Args:
            audio_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file audio
            max_duration: ƒê·ªô d√†i t·ªëi ƒëa (gi√¢y)
            enable_augmentation: B·∫≠t data augmentation (ch·ªâ d√πng khi training)
        """
        # T·∫Øt cache ƒë·ªÉ ti·∫øt ki·ªám VRAM
        if self.enable_cache and audio_path in self._audio_cache:
            return self._audio_cache[audio_path]
        
        try:
            # S·ª≠ d·ª•ng librosa ƒë·ªÉ load audio
            import librosa
            import random
            # Load v·ªõi resampling tr·ª±c ti·∫øp v√† gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·ªÉ ti·∫øt ki·ªám memory
            # max_duration=30s v√¨ Whisper ch·ªâ c·∫ßn 3000 frames (30s * 100 frames/s)
            audio, sr = librosa.load(
                audio_path, 
                sr=16000, 
                mono=True,
                dtype=np.float32,  # S·ª≠ d·ª•ng float32 thay v√¨ float64 ƒë·ªÉ ti·∫øt ki·ªám memory
                duration=max_duration  # Ch·ªâ load t·ªëi ƒëa 30 gi√¢y ƒë·ªÉ ti·∫øt ki·ªám RAM
            )
            
            # ƒê·∫£m b·∫£o audio kh√¥ng qu√° d√†i (an to√†n th√™m)
            max_samples = int(max_duration * sr)
            if len(audio) > max_samples:
                audio = audio[:max_samples]
            
            # Data augmentation (ch·ªâ khi training)
            if enable_augmentation:
                # 1. Volume adjustment (thay ƒë·ªïi √¢m l∆∞·ª£ng)
                if random.random() < 0.3:
                    volume_factor = random.uniform(0.8, 1.2)
                    audio = audio * volume_factor
                
                # 2. Add noise (th√™m nhi·ªÖu nh·∫π)
                if random.random() < 0.2:
                    noise_level = random.uniform(0.005, 0.015)
                    noise = np.random.normal(0, noise_level, len(audio)).astype(np.float32)
                    audio = audio + noise
                    # Clamp ƒë·ªÉ tr√°nh clipping
                    audio = np.clip(audio, -1.0, 1.0)
                
                # 3. Time stretching (thay ƒë·ªïi t·ªëc ƒë·ªô n√≥i) - ch·ªâ √°p d·ª•ng nh·∫π
                if random.random() < 0.2:
                    try:
                        import librosa.effects as effects
                        stretch_factor = random.uniform(0.95, 1.05)  # Thay ƒë·ªïi nh·∫π ¬±5%
                        audio = effects.time_stretch(audio, rate=stretch_factor)
                        # ƒê·∫£m b·∫£o ƒë·ªô d√†i kh√¥ng ƒë·ªïi
                        if len(audio) > max_samples:
                            audio = audio[:max_samples]
                        elif len(audio) < max_samples:
                            # Pad v·ªõi zeros n·∫øu ng·∫Øn h∆°n
                            padding = np.zeros(max_samples - len(audio), dtype=np.float32)
                            audio = np.concatenate([audio, padding])
                    except Exception:
                        pass  # B·ªè qua n·∫øu kh√¥ng th·ªÉ time stretch
            
            # Kh√¥ng cache ƒë·ªÉ ti·∫øt ki·ªám VRAM
            return audio, sr
        except Exception as e:
            print(f"Warning: Kh√¥ng th·ªÉ load audio {audio_path}: {e}")
            # T·∫°o zero array n·∫øu kh√¥ng load ƒë∆∞·ª£c (1 gi√¢y audio)
            return np.zeros(16000, dtype=np.float32), 16000
    
    def __call__(self, features):
        """
        X·ª≠ l√Ω batch: load audio t·ª´ paths v√† convert th√†nh features
        T·ªëi ∆∞u ƒë·ªÉ s·ª≠ d·ª•ng VRAM thay v√¨ RAM
        """
        # T√°ch audio paths v√† sentences t·ª´ features
        if isinstance(features[0], dict):
            audio_paths = [f["audio"] for f in features]
            sentences = [f["sentence"] for f in features]
        else:
            # Fallback n·∫øu format kh√°c
            audio_paths = [str(f.get("audio", "")) for f in features]
            sentences = [str(f.get("sentence", "")) for f in features]
        
        # T·ªëi ∆∞u RAM: Process v√† x√≥a ngay t·ª´ng audio thay v√¨ gi·ªØ t·∫•t c·∫£ trong memory
        # Compute log-Mel input features tr√™n CPU - x·ª≠ l√Ω streaming ƒë·ªÉ ti·∫øt ki·ªám RAM
        processed_features = []
        
        for idx, audio_path in enumerate(audio_paths):
            try:
                # Load audio v·ªõi augmentation n·∫øu ƒë∆∞·ª£c b·∫≠t
                audio_array, sr = self._load_audio(audio_path, enable_augmentation=self.enable_augmentation)
                
                # Extract features ngay l·∫≠p t·ª©c
                features = self.processor.feature_extractor(
                    audio_array,
                    sampling_rate=16000,
                    return_tensors="pt"
                ).input_features  # Shape: [1, n_mels, time_frames]
                
                # X√≥a audio array ngay sau khi extract features ƒë·ªÉ gi·∫£i ph√≥ng RAM
                del audio_array
                
                # ƒê·∫£m b·∫£o c√≥ ƒë√∫ng 3000 frames (Whisper y√™u c·∫ßu)
                current_length = features.shape[-1]
                if current_length < 3000:
                    # Pad v·ªõi zeros ·ªü cu·ªëi
                    padding = torch.zeros(
                        1, 
                        features.shape[1], 
                        3000 - current_length,
                        dtype=features.dtype
                    )
                    features = torch.cat([features, padding], dim=-1)
                elif current_length > 3000:
                    # Truncate ƒë·∫øn 3000
                    features = features[:, :, :3000]
                
                # ƒê·∫£m b·∫£o shape cu·ªëi c√πng l√† [1, n_mels, 3000]
                assert features.shape[-1] == 3000, f"Feature length must be 3000, got {features.shape[-1]}"
                
                # L∆∞u feature ƒë√£ processed (ch·ªâ gi·ªØ feature, kh√¥ng gi·ªØ audio)
                processed_features.append(features.squeeze(0))  # Remove batch dim: [n_mels, 3000]
                
            except Exception as e:
                print(f"Warning: L·ªói khi x·ª≠ l√Ω audio {idx}: {e}")
                # T·∫°o zero features v·ªõi shape ƒë√∫ng [n_mels, 3000]
                n_mels = 80  # Whisper s·ª≠ d·ª•ng 80 mel bins
                zero_features = torch.zeros(n_mels, 3000, dtype=torch.float32)
                processed_features.append(zero_features)
        
        # Stack t·∫•t c·∫£ features l·∫°i th√†nh batch: [batch_size, n_mels, 3000]
        input_features = torch.stack(processed_features, dim=0)
        
        # X√≥a processed_features list ƒë·ªÉ gi·∫£i ph√≥ng RAM
        del processed_features
        
        # Final check: ƒë·∫£m b·∫£o shape ƒë√∫ng
        assert input_features.shape[-1] == 3000, f"All features must have length 3000, got {input_features.shape}"
        
        # Encode labels v·ªõi max_length nh·ªè h∆°n ƒë·ªÉ ti·∫øt ki·ªám VRAM
        labels = self.tokenizer(
            sentences,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=256  # Gi·∫£m t·ª´ 448 xu·ªëng 256 ƒë·ªÉ ti·∫øt ki·ªám VRAM
        ).input_ids
        
        # Replace padding tokens v·ªõi -100 ƒë·ªÉ ignore trong loss
        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id
        labels = labels.masked_fill(labels == pad_token_id, -100)
        
        # Batch - Trainer s·∫Ω t·ª± ƒë·ªông move l√™n GPU khi c·∫ßn
        # Kh√¥ng move ·ªü ƒë√¢y ƒë·ªÉ Trainer c√≥ th·ªÉ qu·∫£n l√Ω device t·ªët h∆°n
        batch = {
            "input_features": input_features,
            "labels": labels
        }
        
        # Force garbage collection ƒë·ªÉ gi·∫£i ph√≥ng RAM ngay l·∫≠p t·ª©c
        import gc
        gc.collect()
        
        return batch


class ClearCacheCallback(TrainerCallback):
    """Callback ƒë·ªÉ clear GPU cache ƒë·ªãnh k·ª≥ (m·ªói N steps ƒë·ªÉ kh√¥ng l√†m ch·∫≠m)"""
    def __init__(self, clear_interval=50):
        self.clear_interval = clear_interval
        self.step_count = 0
    
    def on_step_end(self, args, state, control, **kwargs):
        self.step_count += 1
        # Ch·ªâ clear cache m·ªói N steps ƒë·ªÉ kh√¥ng l√†m ch·∫≠m training
        if self.step_count % self.clear_interval == 0 and torch.cuda.is_available():
            torch.cuda.empty_cache()
        return control


def main():
    parser = argparse.ArgumentParser(description='Fine-tune PhoWhisper model')
    parser.add_argument('--model-name', type=str, default='vinai/PhoWhisper-base',
                        help='PhoWhisper model name (default: vinai/PhoWhisper-base). C√≥ th·ªÉ d√πng: vinai/PhoWhisper-base, vinai/PhoWhisper-large')
    parser.add_argument('--train-jsonl', type=str, default='data/train.jsonl',
                        help='File JSONL training data (default: data/train.jsonl)')
    parser.add_argument('--audio-dir', type=str, default='archive/mp3',
                        help='Th∆∞ m·ª•c ch·ª©a audio files (default: archive/mp3)')
    parser.add_argument('--output-dir', type=str, default='./phowhisper-finetuned',
                        help='Th∆∞ m·ª•c l∆∞u model sau khi fine-tune (default: ./phowhisper-finetuned)')
    parser.add_argument('--num-epochs', type=int, default=5,
                        help='S·ªë epochs (default: 5, khuy·∫øn ngh·ªã 5-10)')
    parser.add_argument('--batch-size', type=int, default=4,
                        help='Batch size (default: 4, ph√π h·ª£p GPU 8GB)')
    parser.add_argument('--learning-rate', type=float, default=1e-5,
                        help='Learning rate (default: 1e-5)')
    parser.add_argument('--warmup-steps', type=int, default=200,
                        help='Warmup steps (default: 200)')
    parser.add_argument('--gradient-accumulation-steps', type=int, default=8,
                        help='Gradient accumulation steps (default: 8, gi·ªØ effective batch nh·ªè tr√™n GPU 8GB)')
    parser.add_argument('--per-device-eval-batch-size', type=int, default=None,
                        help='Batch size cho evaluation (default: gi·ªëng batch size train)')
    parser.add_argument('--fp16', action='store_true', default=None,
                        help='S·ª≠ d·ª•ng mixed precision training (FP16) - M·∫∂C ƒê·ªäNH B·∫¨T khi c√≥ GPU')
    parser.add_argument('--no-fp16', dest='fp16', action='store_false',
                        help='T·∫Øt FP16 (mixed precision training)')
    parser.add_argument('--max-speed', action='store_true', default=None,
                        help='T·ªëi ∆∞u t·ªëi ƒëa t·ªëc ƒë·ªô training (tƒÉng batch size, dataloader). M·∫∑c ƒë·ªãnh t·∫Øt ƒë·ªÉ ti·∫øt ki·ªám VRAM.')
    parser.add_argument('--no-max-speed', dest='max_speed', action='store_false', default=None,
                        help='T·∫Øt ch·∫ø ƒë·ªô t·ªëi ∆∞u t·ªëc ƒë·ªô (d√πng c·∫•u h√¨nh ti·∫øt ki·ªám VRAM)')
    parser.add_argument('--auto-batch-size', action='store_true', default=None,
                        help='T·ª± ƒë·ªông t√¨m batch size t·ªëi ∆∞u d·ª±a tr√™n VRAM (m·∫∑c ƒë·ªãnh t·∫Øt ƒë·ªÉ tr√°nh OOM tr√™n GPU nh·ªè)')
    parser.add_argument('--no-auto-batch-size', dest='auto_batch_size', action='store_false', default=None,
                        help='T·∫Øt t·ª± ƒë·ªông t√¨m batch size t·ªëi ∆∞u')
    parser.add_argument('--eval-jsonl', type=str, default='data/dev.jsonl',
                        help='File JSONL evaluation data (default: data/dev.jsonl, set None ƒë·ªÉ t·∫Øt evaluation)')
    parser.add_argument('--eval-after-train-jsonl', type=str, default='',
                        help='ƒê√°nh gi√° th√™m sau khi train tr√™n JSONL kh√°c (v√≠ d·ª•: data/test.jsonl). B·ªè tr·ªëng ƒë·ªÉ b·ªè qua')
    parser.add_argument('--num-beams', type=int, default=2,
                        help='Beam size khi generate (m·∫∑c ƒë·ªãnh 2)')
    parser.add_argument('--label-smoothing', type=float, default=0.05,
                        help='Label smoothing factor (m·∫∑c ƒë·ªãnh 0.05, khuy·∫øn ngh·ªã 0.0-0.05 cho ASR)')
    parser.add_argument('--no-eval', action='store_true', default=False,
                        help='T·∫Øt evaluation trong v√† sau khi training')
    parser.add_argument('--eval', dest='no_eval', action='store_false', default=True,
                        help='B·∫≠t evaluation (M·∫∂C ƒê·ªäNH B·∫¨T - khuy·∫øn ngh·ªã)')
    parser.add_argument('--use-negative-samples', action='store_true',
                        help='Train c·∫£ negative samples (is_match=False). M·∫∑c ƒë·ªãnh ch·ªâ train positive samples')
    parser.add_argument('--enable-augmentation', action='store_true',
                        help='B·∫≠t data augmentation (volume adjustment, noise, time stretching). M·∫∑c ƒë·ªãnh t·∫Øt ƒë·ªÉ ƒë·∫£m b·∫£o reproducibility')
    parser.add_argument('--dataloader-workers', type=int, default=None,
                        help='S·ªë workers cho dataloader (default: t·ª± ƒë·ªông, 0 khi kh√¥ng c√≥ GPU, 4 khi c√≥ GPU v√† kh√¥ng max-speed, 8 khi max-speed)')
    parser.add_argument('--torch-compile', action='store_true',
                        help='S·ª≠ d·ª•ng torch.compile() ƒë·ªÉ tƒÉng t·ªëc training (PyTorch 2.0+, c√≥ th·ªÉ tƒÉng t·ªëc 20-30%%)')
    parser.add_argument('--eval-steps', type=int, default=None,
                        help='S·ªë steps gi·ªØa m·ªói l·∫ßn evaluation (default: 500, tƒÉng l√™n ƒë·ªÉ ƒë√°nh gi√° √≠t h∆°n v√† train nhanh h∆°n)')
    parser.add_argument('--save-steps', type=int, default=None,
                        help='S·ªë steps gi·ªØa m·ªói l·∫ßn save checkpoint (default: 500, tƒÉng l√™n ƒë·ªÉ save √≠t h∆°n v√† train nhanh h∆°n)')
    
    args = parser.parse_args()
    
    # X√°c ƒë·ªãnh th√¥ng tin GPU ƒë·ªÉ ƒëi·ªÅu ch·ªânh c·∫•u h√¨nh ph√π h·ª£p
    gpu_total_mem_gb = None
    if torch.cuda.is_available():
        try:
            gpu_total_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
        except Exception:
            gpu_total_mem_gb = None
    
    # N·∫øu user kh√¥ng ch·ªâ ƒë·ªãnh, ƒë·∫∑t m·∫∑c ƒë·ªãnh th√¢n thi·ªán v·ªõi GPU 8GB
    if args.fp16 is None:
        args.fp16 = torch.cuda.is_available()
    
    if args.max_speed is None:
        # M·∫∑c ƒë·ªãnh t·∫Øt max-speed ƒë·ªÉ tr√°nh tƒÉng batch size tr√™n GPU nh·ªè
        args.max_speed = False
    
    if args.auto_batch_size is None:
        # M·∫∑c ƒë·ªãnh t·∫Øt auto batch size ƒë·ªÉ tr√°nh th·ª≠ batch qu√° l·ªõn
        args.auto_batch_size = False
    
    # X√°c ƒë·ªãnh s·ªë workers cho dataloader
    if args.dataloader_workers is None:
        if torch.cuda.is_available():
            # M·∫∑c ƒë·ªãnh d√πng 4 workers khi c√≥ GPU (kh√¥ng ph·∫£i max-speed), 8 khi max-speed
            args.dataloader_workers = 8 if args.max_speed else 4
        else:
            # Kh√¥ng d√πng workers khi ch·∫°y tr√™n CPU (c√≥ th·ªÉ ch·∫≠m h∆°n)
            args.dataloader_workers = 0
    
    # Validate model name - ch·ªâ cho ph√©p PhoWhisper models
    valid_phowhisper_models = ['vinai/PhoWhisper-base', 'vinai/PhoWhisper-large']
    if args.model_name not in valid_phowhisper_models:
        print(f"Warning: Model '{args.model_name}' kh√¥ng ph·∫£i l√† PhoWhisper model.")
        print(f"Ch·ªâ h·ªó tr·ª£: {', '.join(valid_phowhisper_models)}")
        print(f"S·ª≠ d·ª•ng default: vinai/PhoWhisper-base")
        args.model_name = 'vinai/PhoWhisper-base'
    
    # Validate file paths
    if not os.path.exists(args.train_jsonl):
        raise FileNotFoundError(
            f"Train JSONL file kh√¥ng t·ªìn t·∫°i: {args.train_jsonl}\n"
            f"H√£y ch·∫°y split_dataset.py tr∆∞·ªõc ƒë·ªÉ t·∫°o dataset, ho·∫∑c ch·ªâ ƒë·ªãnh ƒë√∫ng path v·ªõi --train-jsonl"
        )
    
    # Convert to absolute paths
    args.train_jsonl = os.path.abspath(args.train_jsonl)
    
    # X√°c ƒë·ªãnh audio_dir - t·ª± ƒë·ªông t√¨m n·∫øu kh√¥ng t·ªìn t·∫°i
    audio_dir_abs = os.path.abspath(args.audio_dir)
    if not os.path.exists(audio_dir_abs):
        # Th·ª≠ t√¨m mp3/ ho·∫∑c waves/ trong archive/
        archive_dir = os.path.dirname(audio_dir_abs) if os.path.dirname(audio_dir_abs) else 'archive'
        audio_dir_mp3 = os.path.join(archive_dir, 'mp3')
        audio_dir_waves = os.path.join(archive_dir, 'waves')
        
        if os.path.exists(audio_dir_mp3):
            args.audio_dir = audio_dir_mp3
            print(f"‚ö† Kh√¥ng t√¨m th·∫•y {audio_dir_abs}, t·ª± ƒë·ªông t√¨m th·∫•y mp3/ t·∫°i: {args.audio_dir}")
        elif os.path.exists(audio_dir_waves):
            args.audio_dir = audio_dir_waves
            print(f"‚ö† Kh√¥ng t√¨m th·∫•y {audio_dir_abs}, t·ª± ƒë·ªông t√¨m th·∫•y waves/ t·∫°i: {args.audio_dir}")
        else:
            # Gi·ªØ nguy√™n ƒë·ªÉ hi·ªÉn th·ªã error message r√µ r√†ng
            args.audio_dir = audio_dir_abs
    
    args.audio_dir = os.path.abspath(args.audio_dir)
    
    if args.eval_jsonl and args.eval_jsonl.lower() != 'none':
        if not os.path.exists(args.eval_jsonl):
            print(f"Warning: Eval JSONL file kh√¥ng t·ªìn t·∫°i: {args.eval_jsonl}")
            print(f"S·∫Ω ti·∫øp t·ª•c training kh√¥ng c√≥ evaluation")
            args.eval_jsonl = None
        else:
            args.eval_jsonl = os.path.abspath(args.eval_jsonl)
    else:
        args.eval_jsonl = None
    
    # X·ª≠ l√Ω t·ªëi ∆∞u t·ªëc ƒë·ªô
    if args.max_speed:
        print("\nüöÄ Ch·∫ø ƒë·ªô MAX SPEED ƒë∆∞·ª£c b·∫≠t - T·ªëi ∆∞u t·ªëi ƒëa t·ªëc ƒë·ªô training")
        # T·ª± ƒë·ªông b·∫≠t FP16
        if torch.cuda.is_available() and not args.fp16:
            args.fp16 = True
            print("  ‚úì FP16 (Mixed Precision) ƒë∆∞·ª£c b·∫≠t t·ª± ƒë·ªông")
        elif torch.cuda.is_available() and args.fp16:
            print("  ‚úì FP16 (Mixed Precision) ƒë√£ ƒë∆∞·ª£c b·∫≠t")
        
        # TƒÉng batch size n·∫øu c√≥ th·ªÉ (nh∆∞ng v·∫´n an to√†n)
        original_bs = args.batch_size
        if args.batch_size <= 8:
            args.batch_size = 16
            print(f"  ‚úì Batch size ƒë∆∞·ª£c tƒÉng l√™n: {original_bs} ‚Üí {args.batch_size}")
        elif args.batch_size >= 16:
            print(f"  ‚úì Batch size ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u: {args.batch_size}")
        
        # Gi·∫£m gradient accumulation n·∫øu batch size ƒë√£ tƒÉng
        if args.batch_size >= 16 and args.gradient_accumulation_steps > 2:
            args.gradient_accumulation_steps = 2
            print(f"  ‚úì Gradient accumulation ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh: {args.gradient_accumulation_steps}")
        
        # TƒÉng learning rate m·ªôt ch√∫t ƒë·ªÉ h·ªçc nhanh h∆°n
        if args.learning_rate <= 1e-5:
            args.learning_rate = 1.5e-5
            print(f"  ‚úì Learning rate ƒë∆∞·ª£c tƒÉng l√™n: {args.learning_rate}")
    
    # T·ª± ƒë·ªông t√¨m batch size t·ªëi ∆∞u
    if args.auto_batch_size and torch.cuda.is_available():
        print("\nüîç T·ª± ƒë·ªông t√¨m batch size t·ªëi ∆∞u...")
        original_batch_size = args.batch_size
        optimal_batch_size = original_batch_size
        
        # X√°c ƒë·ªãnh batch size t·ªëi ƒëa d·ª±a tr√™n dung l∆∞·ª£ng GPU
        if gpu_total_mem_gb is not None:
            if gpu_total_mem_gb <= 8.5:
                max_auto_batch = max(4, original_batch_size)
            elif gpu_total_mem_gb <= 12.5:
                max_auto_batch = max(8, original_batch_size)
            else:
                max_auto_batch = max(16, original_batch_size)
        else:
            max_auto_batch = max(8, original_batch_size)
        
        # Th·ª≠ tƒÉng batch size d·∫ßn cho ƒë·∫øn khi ch·∫°m ng∆∞·ª°ng an to√†n
        test_batch_sizes = []
        current_bs = original_batch_size
        while current_bs <= max_auto_batch:
            if current_bs not in test_batch_sizes:
                test_batch_sizes.append(current_bs)
            next_bs = current_bs * 2
            if next_bs <= max_auto_batch and next_bs != current_bs:
                current_bs = next_bs
            else:
                break
        
        if not test_batch_sizes:
            test_batch_sizes = [original_batch_size]
        
        for test_bs in test_batch_sizes:
            try:
                # Test v·ªõi m·ªôt batch nh·ªè
                torch.cuda.empty_cache()
                test_tensor = torch.randn(test_bs, 80, 3000, device='cuda', dtype=torch.float16 if args.fp16 else torch.float32)
                del test_tensor
                torch.cuda.empty_cache()
                optimal_batch_size = test_bs
            except RuntimeError as e:
                if "out of memory" in str(e):
                    torch.cuda.empty_cache()
                    break
                else:
                    raise
        
        if optimal_batch_size > original_batch_size:
            args.batch_size = optimal_batch_size
            print(f"  ‚úì Batch size t·ªëi ∆∞u ƒë∆∞·ª£c t√¨m th·∫•y: {optimal_batch_size} (tƒÉng t·ª´ {original_batch_size})")
        else:
            print(f"  ‚úì Batch size hi·ªán t·∫°i l√† t·ªëi ∆∞u: {original_batch_size}")
    
    # N·∫øu kh√¥ng ch·ªâ ƒë·ªãnh eval batch size, d√πng c√πng batch size train
    if args.per_device_eval_batch_size is None:
        args.per_device_eval_batch_size = args.batch_size
    
    # Ki·ªÉm tra GPU v√† dependencies
    print("\n=== Ki·ªÉm tra m√¥i tr∆∞·ªùng ===")
    
    # Ki·ªÉm tra GPU
    print(f"PyTorch Version: {torch.__version__}")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    
    if torch.cuda.is_available():
        print(f"‚úì GPU ƒë∆∞·ª£c ph√°t hi·ªán!")
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
        print(f"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        if hasattr(torch.version, 'cuda') and torch.version.cuda:
            print(f"  CUDA Version: {torch.version.cuda}")
        print(f"  S·ªë GPU: {torch.cuda.device_count()}")
    else:
        print("‚ö† Warning: Kh√¥ng c√≥ GPU ƒë∆∞·ª£c ph√°t hi·ªán!")
        print("  Training s·∫Ω ch·∫°y tr√™n CPU (r·∫•t ch·∫≠m)")
        print("\n  Ki·ªÉm tra:")
        print("    1. GPU c√≥ ƒë∆∞·ª£c k·∫øt n·ªëi kh√¥ng?")
        print("    2. Driver GPU ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t ch∆∞a? (ch·∫°y: nvidia-smi)")
        print("    3. PyTorch c√≥ h·ªó tr·ª£ CUDA kh√¥ng?")
        
        # Ki·ªÉm tra nvidia-smi
        try:
            import subprocess
            result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                print("\n  ‚ö† nvidia-smi ch·∫°y ƒë∆∞·ª£c nh∆∞ng PyTorch kh√¥ng detect GPU")
                print("  ‚Üí PyTorch kh√¥ng ƒë∆∞·ª£c build v·ªõi CUDA support")
                print("\n  Gi·∫£i ph√°p: C√†i l·∫°i PyTorch v·ªõi CUDA")
                print("  pip uninstall torch torchvision torchaudio")
                print("  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
            else:
                print("\n  ‚ö† Kh√¥ng th·ªÉ ch·∫°y nvidia-smi")
                print("  ‚Üí C√≥ th·ªÉ GPU driver ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        except FileNotFoundError:
            print("\n  ‚ö† Kh√¥ng t√¨m th·∫•y nvidia-smi")
            print("  ‚Üí C√≥ th·ªÉ GPU driver ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t ho·∫∑c kh√¥ng c√≥ NVIDIA GPU")
        except Exception as e:
            print(f"\n  ‚ö† L·ªói khi ki·ªÉm tra nvidia-smi: {e}")
        
        print("\n  N·∫øu kh√¥ng c√≥ GPU, training v·∫´n c√≥ th·ªÉ ch·∫°y tr√™n CPU nh∆∞ng r·∫•t ch·∫≠m")
        # Kh√¥ng h·ªèi user input trong non-interactive mode, ch·ªâ c·∫£nh b√°o
        print("  ‚ö† Ti·∫øp t·ª•c v·ªõi CPU (c√≥ th·ªÉ r·∫•t ch·∫≠m)...")
        print("  üí° Khuy·∫øn ngh·ªã: C√†i PyTorch v·ªõi CUDA ƒë·ªÉ s·ª≠ d·ª•ng GPU")
        print("     pip uninstall torch torchvision torchaudio")
        print("     pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    
    # Ki·ªÉm tra dependencies
    try:
        import librosa
        print(f"‚úì librosa: {librosa.__version__}")
    except ImportError:
        raise ImportError(
            "Thi·∫øu th∆∞ vi·ªán librosa. C√†i ƒë·∫∑t b·∫±ng l·ªánh:\n"
            "pip install librosa soundfile"
        )
    
    try:
        import soundfile
        print(f"‚úì soundfile: {soundfile.__version__}")
    except ImportError:
        raise ImportError(
            "Thi·∫øu th∆∞ vi·ªán soundfile. C√†i ƒë·∫∑t b·∫±ng l·ªánh:\n"
            "pip install soundfile"
        )
    
    print(f"========================\n")
    
    print(f"\n=== C·∫•u h√¨nh Training ===")
    print(f"Model: {args.model_name}")
    print(f"Train JSONL: {args.train_jsonl}")
    print(f"Audio Directory: {args.audio_dir}")
    if args.eval_jsonl:
        print(f"Eval JSONL: {args.eval_jsonl}")
    print(f"Output Directory: {args.output_dir}")
    print(f"Epochs: {args.num_epochs}")
    print(f"Batch Size: {args.batch_size}")
    print(f"Eval Batch Size: {args.per_device_eval_batch_size}")
    print(f"Gradient Accumulation: {args.gradient_accumulation_steps}")
    print(f"Effective Batch Size: {args.batch_size * args.gradient_accumulation_steps}")
    print(f"Learning Rate: {args.learning_rate}")
    print(f"FP16 (Mixed Precision): {args.fp16 and torch.cuda.is_available()}")
    print(f"Dataloader Workers: {args.dataloader_workers}")
    print(f"Dataloader Pin Memory: {torch.cuda.is_available()}")
    print(f"Dataloader Prefetch: {4 if torch.cuda.is_available() else 'None'}")
    if args.torch_compile:
        print(f"üîß Torch Compile: B·∫¨T (c√≥ th·ªÉ tƒÉng t·ªëc 20-30%%)")
    if args.max_speed:
        print(f"üöÄ MAX SPEED Mode: B·∫¨T")
    if args.auto_batch_size:
        print(f"üîç Auto Batch Size: B·∫¨T")
    if args.eval_steps:
        print(f"üìä Eval Steps: {args.eval_steps} (tƒÉng ƒë·ªÉ train nhanh h∆°n)")
    if args.save_steps:
        print(f"üíæ Save Steps: {args.save_steps} (tƒÉng ƒë·ªÉ train nhanh h∆°n)")
    print(f"GPU: {device}")
    print(f"========================\n")
    
    # Load processor v√† model
    print(f"ƒêang load PhoWhisper model: {args.model_name}")
    processor = WhisperProcessor.from_pretrained(args.model_name, language="vi", task="transcribe")
    
    # Load model - Trainer s·∫Ω t·ª± ƒë·ªông move model l√™n GPU n·∫øu c√≥
    model = WhisperForConditionalGeneration.from_pretrained(args.model_name)
    
    # Set language v√† task tokens
    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language="vi", task="transcribe")
    
    # √Åp d·ª•ng torch.compile() ƒë·ªÉ tƒÉng t·ªëc training (PyTorch 2.0+)
    if args.torch_compile and hasattr(torch, 'compile'):
        try:
            print("üîß ƒêang compile model v·ªõi torch.compile() ƒë·ªÉ tƒÉng t·ªëc...")
            # Mode "reduce-overhead" t·ªëi ∆∞u cho training
            model = torch.compile(model, mode="reduce-overhead")
            print("‚úÖ ƒê√£ compile model th√†nh c√¥ng - c√≥ th·ªÉ tƒÉng t·ªëc 20-30%")
        except Exception as e:
            print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ compile model: {e}")
            print("   ‚Üí Ti·∫øp t·ª•c training kh√¥ng compile (c√≥ th·ªÉ ch·∫≠m h∆°n)")
    elif args.torch_compile:
        print("‚ö†Ô∏è  torch.compile() kh√¥ng kh·∫£ d·ª•ng (c·∫ßn PyTorch 2.0+)")
        print("   ‚Üí Ti·∫øp t·ª•c training kh√¥ng compile")
    
    if torch.cuda.is_available():
        print(f"Model s·∫Ω ƒë∆∞·ª£c t·ª± ƒë·ªông chuy·ªÉn l√™n GPU khi training b·∫Øt ƒë·∫ßu")
    else:
        print("Model s·∫Ω ch·∫°y tr√™n CPU")
    
    # Load dataset - KH√îNG preprocess ƒë·ªÉ tr√°nh out of memory
    # S·∫Ω x·ª≠ l√Ω audio on-the-fly trong data collator
    print("ƒêang load dataset (kh√¥ng preprocess ƒë·ªÉ ti·∫øt ki·ªám RAM)...")
    train_dataset = load_jsonl_dataset(args.train_jsonl, args.audio_dir, use_negative_samples=args.use_negative_samples)
    print(f"Train dataset: {len(train_dataset)} samples")
    
    # Load eval dataset n·∫øu c√≥ v√† kh√¥ng t·∫Øt evaluation
    eval_dataset = None
    if args.eval_jsonl and not args.no_eval:
        eval_dataset = load_jsonl_dataset(args.eval_jsonl, args.audio_dir, use_negative_samples=False)
        print(f"Eval dataset: {len(eval_dataset)} samples")
    elif args.no_eval:
        print("Evaluation ƒë√£ ƒë∆∞·ª£c t·∫Øt (--no-eval)")
        args.eval_jsonl = None  # ƒê·∫£m b·∫£o kh√¥ng load eval dataset
    
    print("Dataset s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω on-the-fly trong training ƒë·ªÉ ti·∫øt ki·ªám RAM")
    
    # Training arguments
    training_args = Seq2SeqTrainingArguments(
        output_dir=args.output_dir,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.per_device_eval_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        warmup_steps=args.warmup_steps,
        num_train_epochs=args.num_epochs,
        gradient_checkpointing=True,  # Ti·∫øt ki·ªám VRAM
        fp16=args.fp16 and torch.cuda.is_available(),  # Ch·ªâ d√πng FP16 n·∫øu c√≥ GPU
        bf16=False,  # C√≥ th·ªÉ d√πng bf16 n·∫øu GPU h·ªó tr·ª£ (A100, H100)
        # Dataloader settings: t·ªëi ∆∞u cho t·ªëc ƒë·ªô - lu√¥n b·∫≠t pin_memory v√† prefetch khi c√≥ GPU
        dataloader_num_workers=args.dataloader_workers,
        dataloader_pin_memory=torch.cuda.is_available(),  # Lu√¥n b·∫≠t khi c√≥ GPU ƒë·ªÉ tƒÉng t·ªëc
        dataloader_prefetch_factor=4 if torch.cuda.is_available() else None,  # TƒÉng prefetch ƒë·ªÉ tƒÉng t·ªëc
        eval_strategy="no" if args.no_eval else ("steps" if eval_dataset else "no"),
        eval_steps=None if args.no_eval else (args.eval_steps if args.eval_steps else (500 if eval_dataset else None)),
        save_steps=args.save_steps if args.save_steps else 500,
        logging_steps=100,
        report_to="none",
        load_best_model_at_end=True if eval_dataset else False,
        push_to_hub=False,
        # GPU settings
        remove_unused_columns=False,
        # Optimizations
        optim="adamw_torch",  # S·ª≠ d·ª•ng AdamW optimizer
        max_grad_norm=1.0,  # Gradient clipping
        # Generation for evaluation
        predict_with_generate=True if eval_dataset else False,
        generation_num_beams=max(1, args.num_beams),
    )
    
    # Log GPU memory n·∫øu c√≥
    if torch.cuda.is_available():
        print(f"\n=== GPU Memory Info ===")
        print(f"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
        print(f"GPU Memory Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB")
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        free_memory = total_memory - (torch.cuda.memory_reserved(0) / 1e9)
        print(f"GPU Memory Total: {total_memory:.2f} GB")
        print(f"GPU Memory Free: {free_memory:.2f} GB")
        print(f"========================\n")
    else:
        print("\n‚ö† Training s·∫Ω ch·∫°y tr√™n CPU - r·∫•t ch·∫≠m!")
        print("  Khuy·∫øn ngh·ªã: S·ª≠ d·ª•ng GPU ƒë·ªÉ training nhanh h∆°n\n")
    
    # Custom Data Collator ƒë·ªÉ x·ª≠ l√Ω audio on-the-fly v√† t·ªëi ∆∞u VRAM
    # S·ª≠ d·ª•ng GPU ƒë·ªÉ x·ª≠ l√Ω features v√† gi·∫£m RAM usage
    # T·∫°o data collator v·ªõi device - t·∫Øt cache ƒë·ªÉ ti·∫øt ki·ªám VRAM
    device_for_collator = "cuda" if torch.cuda.is_available() else "cpu"
    data_collator = WhisperDataCollator(
        processor=processor,
        tokenizer=processor.tokenizer,
        padding=True,
        device=device_for_collator,
        enable_cache=False,  # T·∫Øt cache ƒë·ªÉ ti·∫øt ki·ªám VRAM
        enable_augmentation=args.enable_augmentation  # B·∫≠t augmentation n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
    )
    
    # Trainer v·ªõi t·ªëi ∆∞u memory
    trainer = Seq2SeqTrainer(
        args=training_args,
        model=model,
        train_dataset=None,  # s·∫Ω g√°n sau khi ƒë√£ load ƒë·∫ßy ƒë·ªß dataset
        eval_dataset=eval_dataset,
        tokenizer=processor.feature_extractor,
        data_collator=data_collator,
        compute_metrics=lambda pred: compute_metrics(pred, processor) if eval_dataset else None,
    )
    
    # Clear cache tr∆∞·ªõc khi training ƒë·ªÉ gi·∫£i ph√≥ng memory
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("‚úì ƒê√£ clear GPU cache")
    
    import gc
    gc.collect()
    print("‚úì ƒê√£ clear RAM cache")
    print()
    
    # Train v·ªõi callback ƒë·ªÉ clear cache ƒë·ªãnh k·ª≥
    # ClearCacheCallback ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü module level ƒë·ªÉ tr√°nh l·ªói pickle
    # Train tr√™n to√†n b·ªô dataset
    print("B·∫Øt ƒë·∫ßu training...")
    print("L∆∞u √Ω: Audio ƒë∆∞·ª£c load on-the-fly ƒë·ªÉ ti·∫øt ki·ªám RAM, s·ª≠ d·ª•ng VRAM c·ªßa GPU")
    print("\nüí° T·ªëi ∆∞u RAM ƒë√£ ƒë∆∞·ª£c √°p d·ª•ng:")
    print("   - Load v√† x√≥a audio ngay sau khi extract features (streaming processing)")
    print("   - Ch·ªâ load t·ªëi ƒëa 30 gi√¢y audio m·ªói file (ƒë·ªß cho Whisper)")
    print("   - S·ª≠ d·ª•ng float32 thay v√¨ float64")
    print("   - Kh√¥ng cache audio trong RAM")
    print("   - Force garbage collection sau m·ªói batch")
    print("   ‚Üí Gi·∫£m RAM usage ƒë√°ng k·ªÉ so v·ªõi c√°ch load to√†n b·ªô audio tr∆∞·ªõc")
    
    if args.max_speed:
        print("üöÄ Ch·∫ø ƒë·ªô MAX SPEED: T·ªëi ∆∞u t·ªëi ƒëa t·ªëc ƒë·ªô training")
        print(f"   - Batch size: {args.batch_size}")
        print(f"   - FP16: {'B·∫¨T' if args.fp16 else 'T·∫ÆT'}")
        print(f"   - Effective batch size: {args.batch_size * args.gradient_accumulation_steps}")
        print(f"   - Dataloader workers: {args.dataloader_workers} (tƒÉng t·ªëc)")
        if args.torch_compile:
            print(f"   - Torch Compile: B·∫¨T (tƒÉng t·ªëc ~20-30%)")
    else:
        print("‚ö° C√°c t·ªëi ∆∞u ƒë√£ ƒë∆∞·ª£c √°p d·ª•ng:")
        print(f"   - Dataloader workers: {args.dataloader_workers} (load data song song)")
        print(f"   - Pin memory: {'B·∫¨T' if torch.cuda.is_available() else 'T·∫ÆT'} (tƒÉng t·ªëc GPU)")
        print(f"   - Prefetch factor: {4 if torch.cuda.is_available() else 'None'} (load data tr∆∞·ªõc)")
        if args.torch_compile:
            print(f"   - Torch Compile: B·∫¨T (tƒÉng t·ªëc ~20-30%)")
        print("üí° ƒê·ªÉ tƒÉng t·ªëc ƒë·ªô th√™m, th·ª≠:")
        print("   1. D√πng --torch-compile ƒë·ªÉ compile model (tƒÉng t·ªëc 20-30%)")
        print("   2. D√πng --max-speed ƒë·ªÉ t·ª± ƒë·ªông t·ªëi ∆∞u t·ªëi ƒëa")
        print("   3. D√πng --auto-batch-size ƒë·ªÉ t·ª± ƒë·ªông t√¨m batch size t·ªëi ∆∞u")
        print("   4. TƒÉng --eval-steps (v√≠ d·ª•: 1000) ƒë·ªÉ ƒë√°nh gi√° √≠t h∆°n")
        print("   5. TƒÉng --save-steps (v√≠ d·ª•: 1000) ƒë·ªÉ save √≠t h∆°n")
        print("   6. TƒÉng --dataloader-workers l√™n 8 ho·∫∑c 16 n·∫øu c√≥ nhi·ªÅu CPU cores")
        print("‚ö† N·∫øu v·∫´n h·∫øt VRAM, th·ª≠:")
        print("   1. Gi·∫£m batch_size xu·ªëng 4 ho·∫∑c 2: --batch-size 4")
        print("   2. TƒÉng gradient_accumulation_steps: --gradient-accumulation-steps 8")
        print("   3. Gi·∫£m dataloader workers: --dataloader-workers 2")
        print("   4. Set environment variable: set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")

    # Th√™m callback ƒë·ªÉ clear cache m·ªói 50 steps
    trainer.add_callback(ClearCacheCallback(clear_interval=50))

    # Train tr√™n to√†n b·ªô dataset
    trainer.train_dataset = train_dataset
    trainer.train()
    
    # Save model
    print(f"ƒêang l∆∞u model v√†o: {args.output_dir}")
    trainer.save_model()
    trainer.save_state()
    processor.save_pretrained(args.output_dir)
    
    # ƒê√°nh gi√° sau khi train v√† l∆∞u b√°o c√°o (ch·ªâ n·∫øu kh√¥ng t·∫Øt evaluation)
    if not args.no_eval:
        # Load l·∫°i model v·ª´a l∆∞u ƒë·ªÉ ƒë·∫£m b·∫£o ƒë√°nh gi√° s·ª≠ d·ª•ng checkpoint ƒë√£ ƒë∆∞·ª£c ghi ra ƒëƒ©a
        try:
            evaluation_model = WhisperForConditionalGeneration.from_pretrained(args.output_dir)
            evaluation_model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language="vi", task="transcribe")
            print("‚úÖ ƒê√£ load l·∫°i model t·ª´ checkpoint ƒë√£ l∆∞u ƒë·ªÉ ƒë√°nh gi√°")
        except Exception as e:
            print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ load l·∫°i model t·ª´ checkpoint: {e}")
            print("   ‚Üí S·ª≠ d·ª•ng tr·ª±c ti·∫øp model trong b·ªô nh·ªõ ƒë·ªÉ ƒë√°nh gi√°")
            evaluation_model = trainer.model
        
        eval_trainer = Seq2SeqTrainer(
            args=training_args,
            model=evaluation_model,
            train_dataset=None,
            eval_dataset=eval_dataset,
            tokenizer=processor.feature_extractor,
            data_collator=data_collator,
            compute_metrics=lambda pred: compute_metrics(pred, processor) if eval_dataset else None,
        )
        print("\n" + "="*70)
        print("üìä B·∫ÆT ƒê·∫¶U ƒê√ÅNH GI√Å M√î H√åNH")
        print("="*70 + "\n")
        
        results = {}
        if eval_dataset:
            try:
                print("ƒêang ƒë√°nh gi√° tr√™n validation set...")
                eval_metrics = eval_trainer.evaluate()
                results["eval_during_training"] = eval_metrics
                print("‚úÖ Ho√†n th√†nh ƒë√°nh gi√° validation set")
            except Exception as e:
                print(f"‚ö†Ô∏è  Warning: Kh√¥ng th·ªÉ evaluate tr√™n eval_dataset: {e}")

        # ƒê√°nh gi√° b·ªï sung tr√™n JSONL kh√°c (v√≠ d·ª• test)
        if args.eval_after_train_jsonl:
            try:
                extra_path = os.path.abspath(args.eval_after_train_jsonl)
                if os.path.exists(extra_path):
                    print(f"\nƒêang ƒë√°nh gi√° b·ªï sung tr√™n: {os.path.basename(extra_path)}")
                    extra_dataset = load_jsonl_dataset(extra_path, args.audio_dir, use_negative_samples=False)
                    # T·∫°o m·ªôt Trainer t·∫°m ƒë·ªÉ predict tr√™n extra dataset b·∫±ng model ƒë√£ l∆∞u
                    extra_trainer = Seq2SeqTrainer(
                        args=training_args,
                        model=evaluation_model,
                        eval_dataset=extra_dataset,
                        tokenizer=processor.feature_extractor,
                        data_collator=data_collator,
                        compute_metrics=lambda pred: compute_metrics(pred, processor),
                    )
                    extra_metrics = extra_trainer.evaluate(eval_dataset=extra_dataset)
                    results["eval_after_training"] = {
                        "path": extra_path,
                        "metrics": extra_metrics,
                    }
                    print("‚úÖ Ho√†n th√†nh ƒë√°nh gi√° test set")
                else:
                    print(f"‚ö†Ô∏è  Warning: eval_after_train_jsonl kh√¥ng t·ªìn t·∫°i: {extra_path}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Warning: Kh√¥ng th·ªÉ evaluate b·ªï sung: {e}")

        # T·∫°o v√† l∆∞u b√°o c√°o chi ti·∫øt
        if results:
            print("\n" + "="*70)
            print("üìã T·∫†O B√ÅO C√ÅO K·∫æT QU·∫¢")
            print("="*70 + "\n")
            save_detailed_report(results, args.output_dir, args.model_name)
        else:
            print("‚ö†Ô∏è  Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë√°nh gi√° ƒë·ªÉ t·∫°o b√°o c√°o")
    else:
        print("\n‚ö†Ô∏è  Evaluation ƒë√£ ƒë∆∞·ª£c t·∫Øt (--no-eval) - B·ªè qua ƒë√°nh gi√°")

    print("\n" + "="*70)
    print("‚úÖ HO√ÄN TH√ÄNH FINE-TUNING!")
    print("="*70)
    print(f"üìÅ Model ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {args.output_dir}")
    if not args.no_eval:
        print(f"üìä B√°o c√°o ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i:")
        print(f"   - {os.path.join(args.output_dir, 'evaluation_report.txt')}")
        print(f"   - {os.path.join(args.output_dir, 'training_report.json')}")
    print("="*70)


if __name__ == '__main__':
    main()

