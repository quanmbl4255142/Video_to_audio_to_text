"""
Script fine-tuning PhoWhisper vá»›i dá»¯ liá»‡u JSONL

Sá»­ dá»¥ng thÆ° viá»‡n transformers vÃ  datasets tá»« Hugging Face
Chá»‰ há»— trá»£ PhoWhisper models (vinai/PhoWhisper-base, vinai/PhoWhisper-large)
"""

import os
import json
import argparse
from pathlib import Path
import torch
import numpy as np
from datasets import load_dataset, Audio
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)
from transformers.models.whisper.english_normalizer import BasicTextNormalizer
from transformers import TrainerCallback


def load_jsonl_dataset(jsonl_file, audio_dir, use_negative_samples=False):
    """
    Load dataset tá»« JSONL file
    Há»— trá»£ dataset cÃ³ cáº£ positive (is_match=True) vÃ  negative (is_match=False) samples
    
    Args:
        jsonl_file: File JSONL chá»©a {"audio": "path", "sentence": "text", "is_match": bool}
        audio_dir: ThÆ° má»¥c chá»©a audio files (base directory, vÃ­ dá»¥: archive/vivos/train/waves)
        use_negative_samples: Náº¿u True, sáº½ train cáº£ negative samples. Máº·c Ä‘á»‹nh False (chá»‰ train positive)
    
    Returns:
        Dataset vá»›i audio paths Ä‘Ã£ Ä‘Æ°á»£c resolve thÃ nh absolute paths
    """
    # Validate inputs
    if not os.path.exists(jsonl_file):
        raise FileNotFoundError(f"JSONL file khÃ´ng tá»“n táº¡i: {jsonl_file}")
    
    if not os.path.exists(audio_dir):
        raise FileNotFoundError(f"Audio directory khÃ´ng tá»“n táº¡i: {audio_dir}")
    
    print(f"Äang load dataset tá»«: {jsonl_file}")
    print(f"Audio directory: {audio_dir}")
    
    # Äá»c JSONL file
    data = []
    with open(jsonl_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    print(f"Warning: KhÃ´ng thá»ƒ parse dÃ²ng JSON: {line[:50]}... Error: {e}")
    
    print(f"ÄÃ£ Ä‘á»c {len(data)} entries tá»« JSONL")
    
    # Convert audio_dir to absolute path
    audio_dir = os.path.abspath(audio_dir)
    
    # PhÃ¢n loáº¡i vÃ  kiá»ƒm tra entries
    positive_data = []
    negative_data = []
    missing_files = []
    
    for idx, item in enumerate(data):
        if 'audio' not in item or 'sentence' not in item:
            print(f"Warning: Entry {idx} thiáº¿u 'audio' hoáº·c 'sentence': {item}")
            continue
        
        # Kiá»ƒm tra is_match (máº·c Ä‘á»‹nh True náº¿u khÃ´ng cÃ³ field nÃ y - tÆ°Æ¡ng thÃ­ch ngÆ°á»£c)
        is_match = item.get('is_match', True)
        
        audio_path = item['audio']
        # Normalize path: thay backslash báº±ng forward slash (cross-platform)
        audio_path = audio_path.replace('\\', '/')
        
        # Náº¿u lÃ  relative path, thÃªm audio_dir vÃ o
        if not os.path.isabs(audio_path):
            # Join vá»›i audio_dir vÃ  normalize path
            audio_path = os.path.join(audio_dir, audio_path)
            audio_path = os.path.normpath(audio_path)
        else:
            audio_path = os.path.normpath(audio_path)
        
        # Kiá»ƒm tra file cÃ³ tá»“n táº¡i khÃ´ng
        if os.path.exists(audio_path):
            # LÆ°u absolute path
            item['audio'] = os.path.abspath(audio_path)
            if is_match:
                positive_data.append(item)
            else:
                negative_data.append(item)
        else:
            missing_files.append(audio_path)
            if len(missing_files) <= 10:  # Chá»‰ hiá»ƒn thá»‹ 10 file Ä‘áº§u tiÃªn
                print(f"Warning: File khÃ´ng tá»“n táº¡i: {audio_path}")
    
    if missing_files:
        print(f"Warning: Tá»•ng cá»™ng {len(missing_files)} files khÃ´ng tá»“n táº¡i (Ä‘Ã£ hiá»ƒn thá»‹ 10 Ä‘áº§u tiÃªn)")
    
    # Thá»‘ng kÃª
    print(f"\nPhÃ¢n loáº¡i entries:")
    print(f"  - Positive (is_match=True): {len(positive_data)}")
    print(f"  - Negative (is_match=False): {len(negative_data)}")
    
    # Chá»n data Ä‘á»ƒ train
    if use_negative_samples:
        valid_data = positive_data + negative_data
        print(f"  - Sáº½ train trÃªn cáº£ positive vÃ  negative: {len(valid_data)} samples")
    else:
        valid_data = positive_data
        print(f"  - Chá»‰ train trÃªn positive samples: {len(valid_data)} samples")
        if len(negative_data) > 0:
            print(f"  - Bá» qua {len(negative_data)} negative samples (dÃ¹ng --use-negative-samples Ä‘á»ƒ train cáº£ negative)")
    
    if len(valid_data) == 0:
        raise ValueError(f"KhÃ´ng cÃ³ entries há»£p lá»‡ nÃ o Ä‘á»ƒ train! Kiá»ƒm tra láº¡i paths trong JSONL vÃ  audio_dir.")
    
    # Táº¡o dataset tá»« JSONL file vá»›i absolute paths
    # KHÃ”NG load audio vÃ o memory - chá»‰ lÆ°u paths Ä‘á»ƒ xá»­ lÃ½ on-the-fly
    from datasets import Dataset
    from pathlib import Path
    
    # Táº¡o dataset trá»±c tiáº¿p tá»« list of dicts
    # Giá»¯ audio paths dÆ°á»›i dáº¡ng strings (khÃ´ng load audio vÃ o memory)
    print("Äang táº¡o dataset tá»« data (chá»‰ lÆ°u paths, khÃ´ng load audio)...")
    
    # Chuáº©n bá»‹ data vá»›i audio paths dÆ°á»›i dáº¡ng strings
    dataset_data = []
    for item in valid_data:
        # Äáº£m báº£o audio path lÃ  absolute path string
        audio_path = str(Path(item["audio"]).resolve())
        dataset_data.append({
            "audio": audio_path,  # LÆ°u path string, khÃ´ng load audio
            "sentence": item["sentence"]
        })
    
    # Táº¡o Dataset object tá»« list of dicts
    # Audio column lÃ  strings (paths), khÃ´ng pháº£i Audio objects
    dataset = Dataset.from_list(dataset_data)
    
    print("Dataset Ä‘Ã£ sáºµn sÃ ng (audio sáº½ Ä‘Æ°á»£c load on-the-fly trong training)")
    
    return dataset


def prepare_dataset(batch, processor):
    """Chuáº©n bá»‹ dá»¯ liá»‡u cho training"""
    # Load vÃ  resample audio (batch processing)
    audio_arrays = []
    sampling_rates = []
    sentences = []
    
    # Xá»­ lÃ½ tá»«ng item trong batch
    for item in batch["audio"]:
        audio_arrays.append(item["array"])
        sampling_rates.append(item["sampling_rate"])
    
    for sentence in batch["sentence"]:
        sentences.append(sentence)
    
    # Compute log-Mel input features tá»« audio arrays
    inputs = processor.feature_extractor(
        audio_arrays, 
        sampling_rate=sampling_rates[0] if sampling_rates else 16000,
        return_tensors="np"
    ).input_features
    
    # Encode target text thÃ nh label ids
    labels = processor.tokenizer(
        sentences,
        return_tensors="np",
        padding=True,
        truncation=True
    ).input_ids
    
    # Replace padding token id's cá»§a the labels báº±ng -100 Ä‘á»ƒ ignore trong loss
    labels = [
        [(label if label != processor.tokenizer.pad_token_id else -100) for label in label_ids]
        for label_ids in labels
    ]
    
    return {
        "input_features": inputs.tolist(),
        "labels": labels
    }


def compute_wer(reference, hypothesis):
    """TÃ­nh Word Error Rate (WER) sá»­ dá»¥ng dynamic programming"""
    ref_words = reference.strip().lower().split()
    hyp_words = hypothesis.strip().lower().split()
    
    if len(ref_words) == 0:
        return 1.0 if len(hyp_words) > 0 else 0.0
    
    # Dynamic programming Ä‘á»ƒ tÃ­nh edit distance
    n, m = len(ref_words), len(hyp_words)
    dp = np.zeros((n + 1, m + 1), dtype=int)
    
    # Initialize
    for i in range(n + 1):
        dp[i][0] = i  # deletions
    for j in range(m + 1):
        dp[0][j] = j  # insertions
    
    # Fill DP table
    for i in range(1, n + 1):
        for j in range(1, m + 1):
            if ref_words[i-1] == hyp_words[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = min(
                    dp[i-1][j] + 1,      # deletion
                    dp[i][j-1] + 1,      # insertion
                    dp[i-1][j-1] + 1     # substitution
                )
    
    return dp[n][m] / n


def compute_cer(reference, hypothesis):
    """TÃ­nh Character Error Rate (CER)"""
    ref_chars = list(reference.strip().lower().replace(' ', ''))
    hyp_chars = list(hypothesis.strip().lower().replace(' ', ''))
    
    if len(ref_chars) == 0:
        return 1.0 if len(hyp_chars) > 0 else 0.0
    
    n, m = len(ref_chars), len(hyp_chars)
    dp = np.zeros((n + 1, m + 1), dtype=int)
    
    for i in range(n + 1):
        dp[i][0] = i
    for j in range(m + 1):
        dp[0][j] = j
    
    for i in range(1, n + 1):
        for j in range(1, m + 1):
            if ref_chars[i-1] == hyp_chars[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + 1)
    
    return dp[n][m] / n


def compute_metrics(pred, processor):
    """TÃ­nh toÃ¡n cÃ¡c metrics: WER, CER, vÃ  cÃ¡c chá»‰ sá»‘ khÃ¡c"""
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    
    # Decode predictions
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)
    
    # Normalize text
    normalizer = BasicTextNormalizer()
    pred_str = [normalizer(pred) for pred in pred_str]
    label_str = [normalizer(label) for label in label_str]
    
    # TÃ­nh cÃ¡c metrics
    wer_scores = []
    cer_scores = []
    exact_matches = 0
    total_samples = len(pred_str)
    
    for pred, label in zip(pred_str, label_str):
        wer = compute_wer(label, pred)
        cer = compute_cer(label, pred)
        wer_scores.append(wer)
        cer_scores.append(cer)
        
        # Exact match (case-insensitive)
        if pred.strip().lower() == label.strip().lower():
            exact_matches += 1
    
    # TÃ­nh trung bÃ¬nh
    avg_wer = np.mean(wer_scores) if wer_scores else 1.0
    avg_cer = np.mean(cer_scores) if cer_scores else 1.0
    accuracy = exact_matches / total_samples if total_samples > 0 else 0.0
    
    # Word-level accuracy
    total_words = 0
    correct_words = 0
    for pred, label in zip(pred_str, label_str):
        pred_words = pred.strip().lower().split()
        label_words = label.strip().lower().split()
        total_words += len(label_words)
        min_len = min(len(pred_words), len(label_words))
        correct_words += sum(1 for i in range(min_len) if pred_words[i] == label_words[i])
    
    word_accuracy = correct_words / total_words if total_words > 0 else 0.0
    
    return {
        "wer": avg_wer,
        "cer": avg_cer,
        "accuracy": accuracy,
        "word_accuracy": word_accuracy,
        "exact_matches": exact_matches,
        "total_samples": total_samples
    }


def format_metrics_table(metrics_dict, dataset_name="Evaluation"):
    """Táº¡o báº£ng metrics Ä‘áº¹p Ä‘á»ƒ hiá»ƒn thá»‹"""
    lines = []
    lines.append("=" * 70)
    lines.append(f"ğŸ“Š Báº¢NG Káº¾T QUáº¢ ÄÃNH GIÃ: {dataset_name}")
    lines.append("=" * 70)
    
    # Äá»‹nh nghÄ©a cÃ¡c metrics vÃ  mÃ´ táº£
    metric_info = [
        ("WER (Word Error Rate)", "wer", "Tá»· lá»‡ lá»—i tá»« (cÃ ng tháº¥p cÃ ng tá»‘t, 0.0 = hoÃ n háº£o)"),
        ("CER (Character Error Rate)", "cer", "Tá»· lá»‡ lá»—i kÃ½ tá»± (cÃ ng tháº¥p cÃ ng tá»‘t, 0.0 = hoÃ n háº£o)"),
        ("Accuracy (Exact Match)", "accuracy", "Tá»· lá»‡ cÃ¢u chÃ­nh xÃ¡c hoÃ n toÃ n (cÃ ng cao cÃ ng tá»‘t, 1.0 = hoÃ n háº£o)"),
        ("Word Accuracy", "word_accuracy", "Tá»· lá»‡ tá»« chÃ­nh xÃ¡c (cÃ ng cao cÃ ng tá»‘t, 1.0 = hoÃ n háº£o)"),
    ]
    
    # Hiá»ƒn thá»‹ tá»«ng metric
    for metric_name, metric_key, description in metric_info:
        if metric_key in metrics_dict:
            value = metrics_dict[metric_key]
            if isinstance(value, float):
                if metric_key in ["wer", "cer"]:
                    # WER vÃ  CER: hiá»ƒn thá»‹ dÆ°á»›i dáº¡ng pháº§n trÄƒm vÃ  sá»‘ tháº­p phÃ¢n
                    lines.append(f"\n{metric_name}:")
                    lines.append(f"  GiÃ¡ trá»‹: {value:.4f} ({value*100:.2f}%)")
                    lines.append(f"  MÃ´ táº£: {description}")
                    # ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng
                    if value < 0.1:
                        quality = "Xuáº¥t sáº¯c â­â­â­â­â­"
                    elif value < 0.2:
                        quality = "Ráº¥t tá»‘t â­â­â­â­"
                    elif value < 0.3:
                        quality = "Tá»‘t â­â­â­"
                    elif value < 0.5:
                        quality = "KhÃ¡ â­â­"
                    else:
                        quality = "Cáº§n cáº£i thiá»‡n â­"
                    lines.append(f"  ÄÃ¡nh giÃ¡: {quality}")
                else:
                    # Accuracy: hiá»ƒn thá»‹ dÆ°á»›i dáº¡ng pháº§n trÄƒm
                    lines.append(f"\n{metric_name}:")
                    lines.append(f"  GiÃ¡ trá»‹: {value:.4f} ({value*100:.2f}%)")
                    lines.append(f"  MÃ´ táº£: {description}")
                    # ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng
                    if value > 0.9:
                        quality = "Xuáº¥t sáº¯c â­â­â­â­â­"
                    elif value > 0.8:
                        quality = "Ráº¥t tá»‘t â­â­â­â­"
                    elif value > 0.7:
                        quality = "Tá»‘t â­â­â­"
                    elif value > 0.5:
                        quality = "KhÃ¡ â­â­"
                    else:
                        quality = "Cáº§n cáº£i thiá»‡n â­"
                    lines.append(f"  ÄÃ¡nh giÃ¡: {quality}")
    
    # ThÃ´ng tin bá»• sung
    if "exact_matches" in metrics_dict and "total_samples" in metrics_dict:
        exact = metrics_dict["exact_matches"]
        total = metrics_dict["total_samples"]
        lines.append(f"\nğŸ“ˆ Thá»‘ng kÃª:")
        lines.append(f"  Sá»‘ máº«u Ä‘Ã¡nh giÃ¡: {total}")
        lines.append(f"  Sá»‘ cÃ¢u chÃ­nh xÃ¡c hoÃ n toÃ n: {exact}")
        lines.append(f"  Sá»‘ cÃ¢u cÃ³ lá»—i: {total - exact}")
    
    lines.append("=" * 70)
    return "\n".join(lines)


def save_detailed_report(results, output_dir, model_name):
    """LÆ°u bÃ¡o cÃ¡o chi tiáº¿t ra file text vÃ  JSON"""
    import datetime
    
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Táº¡o bÃ¡o cÃ¡o text
    report_lines = []
    report_lines.append("=" * 70)
    report_lines.append("ğŸ“‹ BÃO CÃO Káº¾T QUáº¢ HUáº¤N LUYá»†N MÃ” HÃŒNH")
    report_lines.append("=" * 70)
    report_lines.append(f"Model: {model_name}")
    report_lines.append(f"Thá»i gian: {timestamp}")
    report_lines.append(f"ThÆ° má»¥c output: {output_dir}")
    report_lines.append("")
    
    # ThÃªm tá»«ng pháº§n Ä‘Ã¡nh giÃ¡
    if "eval_during_training" in results:
        report_lines.append(format_metrics_table(
            results["eval_during_training"], 
            "Validation Set (Trong quÃ¡ trÃ¬nh training)"
        ))
        report_lines.append("")
    
    if "eval_after_training" in results:
        eval_info = results["eval_after_training"]
        dataset_path = eval_info.get("path", "Unknown")
        metrics = eval_info.get("metrics", {})
        report_lines.append(format_metrics_table(
            metrics,
            f"Test Set (Sau khi training) - {os.path.basename(dataset_path)}"
        ))
        report_lines.append("")
    
    # Tá»•ng káº¿t
    report_lines.append("=" * 70)
    report_lines.append("ğŸ“Œ Tá»”NG Káº¾T")
    report_lines.append("=" * 70)
    
    best_wer = float('inf')
    best_dataset = None
    
    if "eval_during_training" in results:
        wer = results["eval_during_training"].get("wer", float('inf'))
        if wer < best_wer:
            best_wer = wer
            best_dataset = "Validation Set"
    
    if "eval_after_training" in results:
        wer = results["eval_after_training"].get("metrics", {}).get("wer", float('inf'))
        if wer < best_wer:
            best_wer = wer
            best_dataset = "Test Set"
    
    if best_dataset:
        report_lines.append(f"WER tá»‘t nháº¥t: {best_wer:.4f} ({best_wer*100:.2f}%) trÃªn {best_dataset}")
        if best_wer < 0.1:
            report_lines.append("ğŸ‰ MÃ´ hÃ¬nh Ä‘áº¡t cháº¥t lÆ°á»£ng xuáº¥t sáº¯c!")
        elif best_wer < 0.2:
            report_lines.append("âœ… MÃ´ hÃ¬nh Ä‘áº¡t cháº¥t lÆ°á»£ng ráº¥t tá»‘t!")
        elif best_wer < 0.3:
            report_lines.append("ğŸ‘ MÃ´ hÃ¬nh Ä‘áº¡t cháº¥t lÆ°á»£ng tá»‘t!")
        else:
            report_lines.append("âš ï¸  MÃ´ hÃ¬nh cáº§n Ä‘Æ°á»£c cáº£i thiá»‡n thÃªm.")
    
    report_lines.append("=" * 70)
    
    # LÆ°u file text
    text_report_path = os.path.join(output_dir, "evaluation_report.txt")
    try:
        with open(text_report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report_lines))
        print(f"âœ… ÄÃ£ lÆ°u bÃ¡o cÃ¡o text: {text_report_path}")
    except Exception as e:
        print(f"âš ï¸  KhÃ´ng thá»ƒ lÆ°u bÃ¡o cÃ¡o text: {e}")
    
    # LÆ°u file JSON (Ä‘Ã£ cÃ³ sáºµn trong code chÃ­nh)
    json_report_path = os.path.join(output_dir, "training_report.json")
    try:
        # ThÃªm metadata vÃ o JSON
        json_results = {
            "model_name": model_name,
            "timestamp": timestamp,
            "output_dir": output_dir,
            "results": results
        }
        with open(json_report_path, "w", encoding="utf-8") as f:
            json.dump(json_results, f, ensure_ascii=False, indent=2)
        print(f"âœ… ÄÃ£ lÆ°u bÃ¡o cÃ¡o JSON: {json_report_path}")
    except Exception as e:
        print(f"âš ï¸  KhÃ´ng thá»ƒ lÆ°u bÃ¡o cÃ¡o JSON: {e}")
    
    # In bÃ¡o cÃ¡o ra console
    print("\n" + "\n".join(report_lines))


class WhisperDataCollator:
    """Custom data collator xá»­ lÃ½ audio on-the-fly vÃ  tá»‘i Æ°u VRAM"""
    
    def __init__(self, processor, tokenizer, padding=True, device="cuda", enable_cache=False):
        self.processor = processor
        self.tokenizer = tokenizer
        self.padding = padding
        self.device = device if torch.cuda.is_available() else "cpu"
        # Táº¯t cache Ä‘á»ƒ tiáº¿t kiá»‡m VRAM (cache audio tá»‘n nhiá»u RAM)
        self.enable_cache = enable_cache
        self._audio_cache = {} if enable_cache else {}  # Cache chá»‰ dÃ¹ng khi enable_cache=True
        self._cache_size_limit = 0  # Táº¯t cache Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
    
    def _load_audio(self, audio_path, max_duration=30.0):
        """
        Load audio tá»« file vá»›i error handling - tá»‘i Æ°u RAM
        Chá»‰ load tá»‘i Ä‘a max_duration giÃ¢y (máº·c Ä‘á»‹nh 30s = 3000 frames mel spectrogram)
        """
        # Táº¯t cache Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
        if self.enable_cache and audio_path in self._audio_cache:
            return self._audio_cache[audio_path]
        
        try:
            # Sá»­ dá»¥ng librosa Ä‘á»ƒ load audio
            import librosa
            # Load vá»›i resampling trá»±c tiáº¿p vÃ  giá»›i háº¡n Ä‘á»™ dÃ i Ä‘á»ƒ tiáº¿t kiá»‡m memory
            # max_duration=30s vÃ¬ Whisper chá»‰ cáº§n 3000 frames (30s * 100 frames/s)
            audio, sr = librosa.load(
                audio_path, 
                sr=16000, 
                mono=True,
                dtype=np.float32,  # Sá»­ dá»¥ng float32 thay vÃ¬ float64 Ä‘á»ƒ tiáº¿t kiá»‡m memory
                duration=max_duration  # Chá»‰ load tá»‘i Ä‘a 30 giÃ¢y Ä‘á»ƒ tiáº¿t kiá»‡m RAM
            )
            
            # Äáº£m báº£o audio khÃ´ng quÃ¡ dÃ i (an toÃ n thÃªm)
            max_samples = int(max_duration * sr)
            if len(audio) > max_samples:
                audio = audio[:max_samples]
            
            # KhÃ´ng cache Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
            return audio, sr
        except Exception as e:
            print(f"Warning: KhÃ´ng thá»ƒ load audio {audio_path}: {e}")
            # Táº¡o zero array náº¿u khÃ´ng load Ä‘Æ°á»£c (1 giÃ¢y audio)
            return np.zeros(16000, dtype=np.float32), 16000
    
    def __call__(self, features):
        """
        Xá»­ lÃ½ batch: load audio tá»« paths vÃ  convert thÃ nh features
        Tá»‘i Æ°u Ä‘á»ƒ sá»­ dá»¥ng VRAM thay vÃ¬ RAM
        """
        # TÃ¡ch audio paths vÃ  sentences tá»« features
        if isinstance(features[0], dict):
            audio_paths = [f["audio"] for f in features]
            sentences = [f["sentence"] for f in features]
        else:
            # Fallback náº¿u format khÃ¡c
            audio_paths = [str(f.get("audio", "")) for f in features]
            sentences = [str(f.get("sentence", "")) for f in features]
        
        # Tá»‘i Æ°u RAM: Process vÃ  xÃ³a ngay tá»«ng audio thay vÃ¬ giá»¯ táº¥t cáº£ trong memory
        # Compute log-Mel input features trÃªn CPU - xá»­ lÃ½ streaming Ä‘á»ƒ tiáº¿t kiá»‡m RAM
        processed_features = []
        
        for idx, audio_path in enumerate(audio_paths):
            try:
                # Load audio
                audio_array, sr = self._load_audio(audio_path)
                
                # Extract features ngay láº­p tá»©c
                features = self.processor.feature_extractor(
                    audio_array,
                    sampling_rate=16000,
                    return_tensors="pt"
                ).input_features  # Shape: [1, n_mels, time_frames]
                
                # XÃ³a audio array ngay sau khi extract features Ä‘á»ƒ giáº£i phÃ³ng RAM
                del audio_array
                
                # Äáº£m báº£o cÃ³ Ä‘Ãºng 3000 frames (Whisper yÃªu cáº§u)
                current_length = features.shape[-1]
                if current_length < 3000:
                    # Pad vá»›i zeros á»Ÿ cuá»‘i
                    padding = torch.zeros(
                        1, 
                        features.shape[1], 
                        3000 - current_length,
                        dtype=features.dtype
                    )
                    features = torch.cat([features, padding], dim=-1)
                elif current_length > 3000:
                    # Truncate Ä‘áº¿n 3000
                    features = features[:, :, :3000]
                
                # Äáº£m báº£o shape cuá»‘i cÃ¹ng lÃ  [1, n_mels, 3000]
                assert features.shape[-1] == 3000, f"Feature length must be 3000, got {features.shape[-1]}"
                
                # LÆ°u feature Ä‘Ã£ processed (chá»‰ giá»¯ feature, khÃ´ng giá»¯ audio)
                processed_features.append(features.squeeze(0))  # Remove batch dim: [n_mels, 3000]
                
            except Exception as e:
                print(f"Warning: Lá»—i khi xá»­ lÃ½ audio {idx}: {e}")
                # Táº¡o zero features vá»›i shape Ä‘Ãºng [n_mels, 3000]
                n_mels = 80  # Whisper sá»­ dá»¥ng 80 mel bins
                zero_features = torch.zeros(n_mels, 3000, dtype=torch.float32)
                processed_features.append(zero_features)
        
        # Stack táº¥t cáº£ features láº¡i thÃ nh batch: [batch_size, n_mels, 3000]
        input_features = torch.stack(processed_features, dim=0)
        
        # XÃ³a processed_features list Ä‘á»ƒ giáº£i phÃ³ng RAM
        del processed_features
        
        # Final check: Ä‘áº£m báº£o shape Ä‘Ãºng
        assert input_features.shape[-1] == 3000, f"All features must have length 3000, got {input_features.shape}"
        
        # Encode labels vá»›i max_length nhá» hÆ¡n Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
        labels = self.tokenizer(
            sentences,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=256  # Giáº£m tá»« 448 xuá»‘ng 256 Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
        ).input_ids
        
        # Replace padding tokens vá»›i -100 Ä‘á»ƒ ignore trong loss
        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id
        labels = labels.masked_fill(labels == pad_token_id, -100)
        
        # Batch - Trainer sáº½ tá»± Ä‘á»™ng move lÃªn GPU khi cáº§n
        # KhÃ´ng move á»Ÿ Ä‘Ã¢y Ä‘á»ƒ Trainer cÃ³ thá»ƒ quáº£n lÃ½ device tá»‘t hÆ¡n
        batch = {
            "input_features": input_features,
            "labels": labels
        }
        
        # Force garbage collection Ä‘á»ƒ giáº£i phÃ³ng RAM ngay láº­p tá»©c
        import gc
        gc.collect()
        
        return batch


class ClearCacheCallback(TrainerCallback):
    """Callback Ä‘á»ƒ clear GPU cache Ä‘á»‹nh ká»³ (má»—i N steps Ä‘á»ƒ khÃ´ng lÃ m cháº­m)"""
    def __init__(self, clear_interval=50):
        self.clear_interval = clear_interval
        self.step_count = 0
    
    def on_step_end(self, args, state, control, **kwargs):
        self.step_count += 1
        # Chá»‰ clear cache má»—i N steps Ä‘á»ƒ khÃ´ng lÃ m cháº­m training
        if self.step_count % self.clear_interval == 0 and torch.cuda.is_available():
            torch.cuda.empty_cache()
        return control


def main():
    parser = argparse.ArgumentParser(description='Fine-tune PhoWhisper model')
    parser.add_argument('--model-name', type=str, default='vinai/PhoWhisper-base',
                        help='PhoWhisper model name (default: vinai/PhoWhisper-base). CÃ³ thá»ƒ dÃ¹ng: vinai/PhoWhisper-base, vinai/PhoWhisper-large')
    parser.add_argument('--train-jsonl', type=str, default='data/train.jsonl',
                        help='File JSONL training data (default: data/train.jsonl)')
    parser.add_argument('--audio-dir', type=str, default='archive/vivos/train/waves',
                        help='ThÆ° má»¥c chá»©a audio files (default: archive/vivos/train/waves)')
    parser.add_argument('--output-dir', type=str, default='./phowhisper-finetuned',
                        help='ThÆ° má»¥c lÆ°u model sau khi fine-tune (default: ./phowhisper-finetuned)')
    parser.add_argument('--num-epochs', type=int, default=3,
                        help='Sá»‘ epochs (default: 3)')
    parser.add_argument('--batch-size', type=int, default=4,
                        help='Batch size (default: 4, phÃ¹ há»£p GPU 8GB)')
    parser.add_argument('--learning-rate', type=float, default=1e-5,
                        help='Learning rate (default: 1e-5)')
    parser.add_argument('--warmup-steps', type=int, default=200,
                        help='Warmup steps (default: 200)')
    parser.add_argument('--gradient-accumulation-steps', type=int, default=8,
                        help='Gradient accumulation steps (default: 8, giá»¯ effective batch nhá» trÃªn GPU 8GB)')
    parser.add_argument('--per-device-eval-batch-size', type=int, default=None,
                        help='Batch size cho evaluation (default: giá»‘ng batch size train)')
    parser.add_argument('--fp16', action='store_true', default=None,
                        help='Sá»­ dá»¥ng mixed precision training (FP16) - Máº¶C Äá»ŠNH Báº¬T khi cÃ³ GPU')
    parser.add_argument('--no-fp16', dest='fp16', action='store_false',
                        help='Táº¯t FP16 (mixed precision training)')
    parser.add_argument('--max-speed', action='store_true', default=None,
                        help='Tá»‘i Æ°u tá»‘i Ä‘a tá»‘c Ä‘á»™ training (tÄƒng batch size, dataloader). Máº·c Ä‘á»‹nh táº¯t Ä‘á»ƒ tiáº¿t kiá»‡m VRAM.')
    parser.add_argument('--no-max-speed', dest='max_speed', action='store_false', default=None,
                        help='Táº¯t cháº¿ Ä‘á»™ tá»‘i Æ°u tá»‘c Ä‘á»™ (dÃ¹ng cáº¥u hÃ¬nh tiáº¿t kiá»‡m VRAM)')
    parser.add_argument('--auto-batch-size', action='store_true', default=None,
                        help='Tá»± Ä‘á»™ng tÃ¬m batch size tá»‘i Æ°u dá»±a trÃªn VRAM (máº·c Ä‘á»‹nh táº¯t Ä‘á»ƒ trÃ¡nh OOM trÃªn GPU nhá»)')
    parser.add_argument('--no-auto-batch-size', dest='auto_batch_size', action='store_false', default=None,
                        help='Táº¯t tá»± Ä‘á»™ng tÃ¬m batch size tá»‘i Æ°u')
    parser.add_argument('--eval-jsonl', type=str, default='data/dev.jsonl',
                        help='File JSONL evaluation data (default: data/dev.jsonl, set None Ä‘á»ƒ táº¯t evaluation)')
    parser.add_argument('--eval-after-train-jsonl', type=str, default='',
                        help='ÄÃ¡nh giÃ¡ thÃªm sau khi train trÃªn JSONL khÃ¡c (vÃ­ dá»¥: data/test.jsonl). Bá» trá»‘ng Ä‘á»ƒ bá» qua')
    parser.add_argument('--num-beams', type=int, default=2,
                        help='Beam size khi generate (máº·c Ä‘á»‹nh 2)')
    parser.add_argument('--label-smoothing', type=float, default=0.1,
                        help='Label smoothing factor (máº·c Ä‘á»‹nh 0.1)')
    parser.add_argument('--chunk-size', type=int, default=1000,
                        help='Sá»‘ máº«u má»—i láº§n train theo tá»«ng Ä‘á»£t (máº·c Ä‘á»‹nh: 1000, 0 = táº¯t chunked training)')
    parser.add_argument('--start-chunk', type=int, default=0,
                        help='Báº¯t Ä‘áº§u tá»« chunk index nÃ o (máº·c Ä‘á»‹nh 0)')
    parser.add_argument('--no-eval', action='store_true', default=True,
                        help='Táº¯t evaluation trong vÃ  sau khi training (Máº¶C Äá»ŠNH Báº¬T)')
    parser.add_argument('--eval', dest='no_eval', action='store_false',
                        help='Báº­t evaluation (táº¯t --no-eval)')
    parser.add_argument('--use-negative-samples', action='store_true',
                        help='Train cáº£ negative samples (is_match=False). Máº·c Ä‘á»‹nh chá»‰ train positive samples')
    
    args = parser.parse_args()
    
    # XÃ¡c Ä‘á»‹nh thÃ´ng tin GPU Ä‘á»ƒ Ä‘iá»u chá»‰nh cáº¥u hÃ¬nh phÃ¹ há»£p
    gpu_total_mem_gb = None
    if torch.cuda.is_available():
        try:
            gpu_total_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
        except Exception:
            gpu_total_mem_gb = None
    
    # Náº¿u user khÃ´ng chá»‰ Ä‘á»‹nh, Ä‘áº·t máº·c Ä‘á»‹nh thÃ¢n thiá»‡n vá»›i GPU 8GB
    if args.fp16 is None:
        args.fp16 = torch.cuda.is_available()
    
    if args.max_speed is None:
        # Máº·c Ä‘á»‹nh táº¯t max-speed Ä‘á»ƒ trÃ¡nh tÄƒng batch size trÃªn GPU nhá»
        args.max_speed = False
    
    if args.auto_batch_size is None:
        # Máº·c Ä‘á»‹nh táº¯t auto batch size Ä‘á»ƒ trÃ¡nh thá»­ batch quÃ¡ lá»›n
        args.auto_batch_size = False
    
    # Validate model name - chá»‰ cho phÃ©p PhoWhisper models
    valid_phowhisper_models = ['vinai/PhoWhisper-base', 'vinai/PhoWhisper-large']
    if args.model_name not in valid_phowhisper_models:
        print(f"Warning: Model '{args.model_name}' khÃ´ng pháº£i lÃ  PhoWhisper model.")
        print(f"Chá»‰ há»— trá»£: {', '.join(valid_phowhisper_models)}")
        print(f"Sá»­ dá»¥ng default: vinai/PhoWhisper-base")
        args.model_name = 'vinai/PhoWhisper-base'
    
    # Validate file paths
    if not os.path.exists(args.train_jsonl):
        raise FileNotFoundError(
            f"Train JSONL file khÃ´ng tá»“n táº¡i: {args.train_jsonl}\n"
            f"HÃ£y cháº¡y split_dataset.py trÆ°á»›c Ä‘á»ƒ táº¡o dataset, hoáº·c chá»‰ Ä‘á»‹nh Ä‘Ãºng path vá»›i --train-jsonl"
        )
    
    # Convert to absolute paths
    args.train_jsonl = os.path.abspath(args.train_jsonl)
    args.audio_dir = os.path.abspath(args.audio_dir)
    
    if args.eval_jsonl and args.eval_jsonl.lower() != 'none':
        if not os.path.exists(args.eval_jsonl):
            print(f"Warning: Eval JSONL file khÃ´ng tá»“n táº¡i: {args.eval_jsonl}")
            print(f"Sáº½ tiáº¿p tá»¥c training khÃ´ng cÃ³ evaluation")
            args.eval_jsonl = None
        else:
            args.eval_jsonl = os.path.abspath(args.eval_jsonl)
    else:
        args.eval_jsonl = None
    
    # Xá»­ lÃ½ tá»‘i Æ°u tá»‘c Ä‘á»™
    if args.max_speed:
        print("\nğŸš€ Cháº¿ Ä‘á»™ MAX SPEED Ä‘Æ°á»£c báº­t - Tá»‘i Æ°u tá»‘i Ä‘a tá»‘c Ä‘á»™ training")
        # Tá»± Ä‘á»™ng báº­t FP16
        if torch.cuda.is_available() and not args.fp16:
            args.fp16 = True
            print("  âœ“ FP16 (Mixed Precision) Ä‘Æ°á»£c báº­t tá»± Ä‘á»™ng")
        elif torch.cuda.is_available() and args.fp16:
            print("  âœ“ FP16 (Mixed Precision) Ä‘Ã£ Ä‘Æ°á»£c báº­t")
        
        # TÄƒng batch size náº¿u cÃ³ thá»ƒ (nhÆ°ng váº«n an toÃ n)
        original_bs = args.batch_size
        if args.batch_size <= 8:
            args.batch_size = 16
            print(f"  âœ“ Batch size Ä‘Æ°á»£c tÄƒng lÃªn: {original_bs} â†’ {args.batch_size}")
        elif args.batch_size >= 16:
            print(f"  âœ“ Batch size Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u: {args.batch_size}")
        
        # Giáº£m gradient accumulation náº¿u batch size Ä‘Ã£ tÄƒng
        if args.batch_size >= 16 and args.gradient_accumulation_steps > 2:
            args.gradient_accumulation_steps = 2
            print(f"  âœ“ Gradient accumulation Ä‘Æ°á»£c Ä‘iá»u chá»‰nh: {args.gradient_accumulation_steps}")
        
        # TÄƒng learning rate má»™t chÃºt Ä‘á»ƒ há»c nhanh hÆ¡n
        if args.learning_rate <= 1e-5:
            args.learning_rate = 1.5e-5
            print(f"  âœ“ Learning rate Ä‘Æ°á»£c tÄƒng lÃªn: {args.learning_rate}")
    
    # Tá»± Ä‘á»™ng tÃ¬m batch size tá»‘i Æ°u
    if args.auto_batch_size and torch.cuda.is_available():
        print("\nğŸ” Tá»± Ä‘á»™ng tÃ¬m batch size tá»‘i Æ°u...")
        original_batch_size = args.batch_size
        optimal_batch_size = original_batch_size
        
        # XÃ¡c Ä‘á»‹nh batch size tá»‘i Ä‘a dá»±a trÃªn dung lÆ°á»£ng GPU
        if gpu_total_mem_gb is not None:
            if gpu_total_mem_gb <= 8.5:
                max_auto_batch = max(4, original_batch_size)
            elif gpu_total_mem_gb <= 12.5:
                max_auto_batch = max(8, original_batch_size)
            else:
                max_auto_batch = max(16, original_batch_size)
        else:
            max_auto_batch = max(8, original_batch_size)
        
        # Thá»­ tÄƒng batch size dáº§n cho Ä‘áº¿n khi cháº¡m ngÆ°á»¡ng an toÃ n
        test_batch_sizes = []
        current_bs = original_batch_size
        while current_bs <= max_auto_batch:
            if current_bs not in test_batch_sizes:
                test_batch_sizes.append(current_bs)
            next_bs = current_bs * 2
            if next_bs <= max_auto_batch and next_bs != current_bs:
                current_bs = next_bs
            else:
                break
        
        if not test_batch_sizes:
            test_batch_sizes = [original_batch_size]
        
        for test_bs in test_batch_sizes:
            try:
                # Test vá»›i má»™t batch nhá»
                torch.cuda.empty_cache()
                test_tensor = torch.randn(test_bs, 80, 3000, device='cuda', dtype=torch.float16 if args.fp16 else torch.float32)
                del test_tensor
                torch.cuda.empty_cache()
                optimal_batch_size = test_bs
            except RuntimeError as e:
                if "out of memory" in str(e):
                    torch.cuda.empty_cache()
                    break
                else:
                    raise
        
        if optimal_batch_size > original_batch_size:
            args.batch_size = optimal_batch_size
            print(f"  âœ“ Batch size tá»‘i Æ°u Ä‘Æ°á»£c tÃ¬m tháº¥y: {optimal_batch_size} (tÄƒng tá»« {original_batch_size})")
        else:
            print(f"  âœ“ Batch size hiá»‡n táº¡i lÃ  tá»‘i Æ°u: {original_batch_size}")
    
    # Náº¿u khÃ´ng chá»‰ Ä‘á»‹nh eval batch size, dÃ¹ng cÃ¹ng batch size train
    if args.per_device_eval_batch_size is None:
        args.per_device_eval_batch_size = args.batch_size
    
    # Kiá»ƒm tra GPU vÃ  dependencies
    print("\n=== Kiá»ƒm tra mÃ´i trÆ°á»ng ===")
    
    # Kiá»ƒm tra GPU
    print(f"PyTorch Version: {torch.__version__}")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    
    if torch.cuda.is_available():
        print(f"âœ“ GPU Ä‘Æ°á»£c phÃ¡t hiá»‡n!")
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
        print(f"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        if hasattr(torch.version, 'cuda') and torch.version.cuda:
            print(f"  CUDA Version: {torch.version.cuda}")
        print(f"  Sá»‘ GPU: {torch.cuda.device_count()}")
    else:
        print("âš  Warning: KhÃ´ng cÃ³ GPU Ä‘Æ°á»£c phÃ¡t hiá»‡n!")
        print("  Training sáº½ cháº¡y trÃªn CPU (ráº¥t cháº­m)")
        print("\n  Kiá»ƒm tra:")
        print("    1. GPU cÃ³ Ä‘Æ°á»£c káº¿t ná»‘i khÃ´ng?")
        print("    2. Driver GPU Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t chÆ°a? (cháº¡y: nvidia-smi)")
        print("    3. PyTorch cÃ³ há»— trá»£ CUDA khÃ´ng?")
        
        # Kiá»ƒm tra nvidia-smi
        try:
            import subprocess
            result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                print("\n  âš  nvidia-smi cháº¡y Ä‘Æ°á»£c nhÆ°ng PyTorch khÃ´ng detect GPU")
                print("  â†’ PyTorch khÃ´ng Ä‘Æ°á»£c build vá»›i CUDA support")
                print("\n  Giáº£i phÃ¡p: CÃ i láº¡i PyTorch vá»›i CUDA")
                print("  pip uninstall torch torchvision torchaudio")
                print("  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
            else:
                print("\n  âš  KhÃ´ng thá»ƒ cháº¡y nvidia-smi")
                print("  â†’ CÃ³ thá»ƒ GPU driver chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t")
        except FileNotFoundError:
            print("\n  âš  KhÃ´ng tÃ¬m tháº¥y nvidia-smi")
            print("  â†’ CÃ³ thá»ƒ GPU driver chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t hoáº·c khÃ´ng cÃ³ NVIDIA GPU")
        except Exception as e:
            print(f"\n  âš  Lá»—i khi kiá»ƒm tra nvidia-smi: {e}")
        
        print("\n  Náº¿u khÃ´ng cÃ³ GPU, training váº«n cÃ³ thá»ƒ cháº¡y trÃªn CPU nhÆ°ng ráº¥t cháº­m")
        # KhÃ´ng há»i user input trong non-interactive mode, chá»‰ cáº£nh bÃ¡o
        print("  âš  Tiáº¿p tá»¥c vá»›i CPU (cÃ³ thá»ƒ ráº¥t cháº­m)...")
        print("  ğŸ’¡ Khuyáº¿n nghá»‹: CÃ i PyTorch vá»›i CUDA Ä‘á»ƒ sá»­ dá»¥ng GPU")
        print("     pip uninstall torch torchvision torchaudio")
        print("     pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    
    # Kiá»ƒm tra dependencies
    try:
        import librosa
        print(f"âœ“ librosa: {librosa.__version__}")
    except ImportError:
        raise ImportError(
            "Thiáº¿u thÆ° viá»‡n librosa. CÃ i Ä‘áº·t báº±ng lá»‡nh:\n"
            "pip install librosa soundfile"
        )
    
    try:
        import soundfile
        print(f"âœ“ soundfile: {soundfile.__version__}")
    except ImportError:
        raise ImportError(
            "Thiáº¿u thÆ° viá»‡n soundfile. CÃ i Ä‘áº·t báº±ng lá»‡nh:\n"
            "pip install soundfile"
        )
    
    print(f"========================\n")
    
    print(f"\n=== Cáº¥u hÃ¬nh Training ===")
    print(f"Model: {args.model_name}")
    print(f"Train JSONL: {args.train_jsonl}")
    print(f"Audio Directory: {args.audio_dir}")
    if args.eval_jsonl:
        print(f"Eval JSONL: {args.eval_jsonl}")
    print(f"Output Directory: {args.output_dir}")
    print(f"Epochs: {args.num_epochs}")
    print(f"Batch Size: {args.batch_size}")
    print(f"Eval Batch Size: {args.per_device_eval_batch_size}")
    print(f"Gradient Accumulation: {args.gradient_accumulation_steps}")
    print(f"Effective Batch Size: {args.batch_size * args.gradient_accumulation_steps}")
    print(f"Learning Rate: {args.learning_rate}")
    print(f"FP16 (Mixed Precision): {args.fp16 and torch.cuda.is_available()}")
    if args.max_speed:
        print(f"ğŸš€ MAX SPEED Mode: Báº¬T")
    if args.auto_batch_size:
        print(f"ğŸ” Auto Batch Size: Báº¬T")
    print(f"GPU: {device}")
    print(f"========================\n")
    
    # Load processor vÃ  model
    print(f"Äang load PhoWhisper model: {args.model_name}")
    processor = WhisperProcessor.from_pretrained(args.model_name, language="vi", task="transcribe")
    
    # Load model - Trainer sáº½ tá»± Ä‘á»™ng move model lÃªn GPU náº¿u cÃ³
    model = WhisperForConditionalGeneration.from_pretrained(args.model_name)
    
    # Set language vÃ  task tokens
    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language="vi", task="transcribe")
    
    if torch.cuda.is_available():
        print(f"Model sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng chuyá»ƒn lÃªn GPU khi training báº¯t Ä‘áº§u")
    else:
        print("Model sáº½ cháº¡y trÃªn CPU")
    
    # Load dataset - KHÃ”NG preprocess Ä‘á»ƒ trÃ¡nh out of memory
    # Sáº½ xá»­ lÃ½ audio on-the-fly trong data collator
    print("Äang load dataset (khÃ´ng preprocess Ä‘á»ƒ tiáº¿t kiá»‡m RAM)...")
    train_dataset = load_jsonl_dataset(args.train_jsonl, args.audio_dir, use_negative_samples=args.use_negative_samples)
    print(f"Train dataset: {len(train_dataset)} samples")
    
    # Load eval dataset náº¿u cÃ³ vÃ  khÃ´ng táº¯t evaluation
    eval_dataset = None
    if args.eval_jsonl and not args.no_eval:
        eval_dataset = load_jsonl_dataset(args.eval_jsonl, args.audio_dir, use_negative_samples=False)
        print(f"Eval dataset: {len(eval_dataset)} samples")
    elif args.no_eval:
        print("Evaluation Ä‘Ã£ Ä‘Æ°á»£c táº¯t (--no-eval)")
        args.eval_jsonl = None  # Äáº£m báº£o khÃ´ng load eval dataset
    
    print("Dataset sáº½ Ä‘Æ°á»£c xá»­ lÃ½ on-the-fly trong training Ä‘á»ƒ tiáº¿t kiá»‡m RAM")
    
    # Training arguments
    training_args = Seq2SeqTrainingArguments(
        output_dir=args.output_dir,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.per_device_eval_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        warmup_steps=args.warmup_steps,
        num_train_epochs=args.num_epochs,
        gradient_checkpointing=True,  # Tiáº¿t kiá»‡m VRAM
        fp16=args.fp16 and torch.cuda.is_available(),  # Chá»‰ dÃ¹ng FP16 náº¿u cÃ³ GPU
        bf16=False,  # CÃ³ thá»ƒ dÃ¹ng bf16 náº¿u GPU há»— trá»£ (A100, H100)
        # Dataloader settings: tá»‘i Æ°u cho tá»‘c Ä‘á»™ náº¿u max-speed, ngÆ°á»£c láº¡i tá»‘i Æ°u VRAM
        dataloader_num_workers=2 if (args.max_speed and torch.cuda.is_available()) else 0,
        dataloader_pin_memory=True if (args.max_speed and torch.cuda.is_available()) else False,
        dataloader_prefetch_factor=2 if (args.max_speed and torch.cuda.is_available()) else None,
        eval_strategy="no" if args.no_eval else ("steps" if eval_dataset else "no"),
        eval_steps=None if args.no_eval else (500 if eval_dataset else None),
        save_steps=500,
        logging_steps=100,
        report_to="none",
        load_best_model_at_end=True if eval_dataset else False,
        push_to_hub=False,
        # GPU settings
        remove_unused_columns=False,
        # Optimizations
        optim="adamw_torch",  # Sá»­ dá»¥ng AdamW optimizer
        max_grad_norm=1.0,  # Gradient clipping
        # Generation for evaluation
        predict_with_generate=True if eval_dataset else False,
        generation_num_beams=max(1, args.num_beams),
    )
    
    # Log GPU memory náº¿u cÃ³
    if torch.cuda.is_available():
        print(f"\n=== GPU Memory Info ===")
        print(f"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
        print(f"GPU Memory Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB")
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        free_memory = total_memory - (torch.cuda.memory_reserved(0) / 1e9)
        print(f"GPU Memory Total: {total_memory:.2f} GB")
        print(f"GPU Memory Free: {free_memory:.2f} GB")
        print(f"========================\n")
    else:
        print("\nâš  Training sáº½ cháº¡y trÃªn CPU - ráº¥t cháº­m!")
        print("  Khuyáº¿n nghá»‹: Sá»­ dá»¥ng GPU Ä‘á»ƒ training nhanh hÆ¡n\n")
    
    # Custom Data Collator Ä‘á»ƒ xá»­ lÃ½ audio on-the-fly vÃ  tá»‘i Æ°u VRAM
    # Sá»­ dá»¥ng GPU Ä‘á»ƒ xá»­ lÃ½ features vÃ  giáº£m RAM usage
    # Táº¡o data collator vá»›i device - táº¯t cache Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
    device_for_collator = "cuda" if torch.cuda.is_available() else "cpu"
    data_collator = WhisperDataCollator(
        processor=processor,
        tokenizer=processor.tokenizer,
        padding=True,
        device=device_for_collator,
        enable_cache=False  # Táº¯t cache Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
    )
    
    # Trainer vá»›i tá»‘i Æ°u memory
    trainer = Seq2SeqTrainer(
        args=training_args,
        model=model,
        train_dataset=None,  # sáº½ gÃ¡n sau (há»— trá»£ train theo tá»«ng chunk)
        eval_dataset=eval_dataset,
        tokenizer=processor.feature_extractor,
        data_collator=data_collator,
        compute_metrics=lambda pred: compute_metrics(pred, processor) if eval_dataset else None,
    )
    
    # Clear cache trÆ°á»›c khi training Ä‘á»ƒ giáº£i phÃ³ng memory
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("âœ“ ÄÃ£ clear GPU cache")
    
    import gc
    gc.collect()
    print("âœ“ ÄÃ£ clear RAM cache")
    print()
    
    # Train vá»›i callback Ä‘á»ƒ clear cache Ä‘á»‹nh ká»³
    # ClearCacheCallback Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a á»Ÿ module level Ä‘á»ƒ trÃ¡nh lá»—i pickle
    # Train (há»— trá»£ train theo tá»«ng Ä‘á»£t/chunk)
    print("Báº¯t Ä‘áº§u training...")
    print("LÆ°u Ã½: Audio Ä‘Æ°á»£c load on-the-fly Ä‘á»ƒ tiáº¿t kiá»‡m RAM, sá»­ dá»¥ng VRAM cá»§a GPU")
    print("\nğŸ’¡ Tá»‘i Æ°u RAM Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng:")
    print("   - Load vÃ  xÃ³a audio ngay sau khi extract features (streaming processing)")
    print("   - Chá»‰ load tá»‘i Ä‘a 30 giÃ¢y audio má»—i file (Ä‘á»§ cho Whisper)")
    print("   - Sá»­ dá»¥ng float32 thay vÃ¬ float64")
    print("   - KhÃ´ng cache audio trong RAM")
    print("   - Force garbage collection sau má»—i batch")
    print("   â†’ Giáº£m RAM usage Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡ch load toÃ n bá»™ audio trÆ°á»›c")
    
    if args.max_speed:
        print("ğŸš€ Cháº¿ Ä‘á»™ MAX SPEED: Tá»‘i Æ°u tá»‘i Ä‘a tá»‘c Ä‘á»™ training")
        print(f"   - Batch size: {args.batch_size}")
        print(f"   - FP16: {'Báº¬T' if args.fp16 else 'Táº®T'}")
        print(f"   - Effective batch size: {args.batch_size * args.gradient_accumulation_steps}")
        print(f"   - Dataloader workers: 2 (tÄƒng tá»‘c)")
    else:
        print("âš  Tá»‘i Æ°u VRAM: batch_size=8, gradient_accumulation=4, cache táº¯t, workers=0")
        print("âš  Náº¿u muá»‘n tÄƒng tá»‘c Ä‘á»™, thá»­:")
        print("   1. DÃ¹ng --max-speed Ä‘á»ƒ tá»± Ä‘á»™ng tá»‘i Æ°u")
        print("   2. DÃ¹ng --auto-batch-size Ä‘á»ƒ tá»± Ä‘á»™ng tÃ¬m batch size tá»‘i Æ°u")
        print("   3. Báº­t --fp16 Ä‘á»ƒ tÄƒng tá»‘c")
        print("âš  Náº¿u váº«n háº¿t VRAM, thá»­:")
        print("   1. Giáº£m batch_size xuá»‘ng 4 hoáº·c 2: --batch-size 4")
        print("   2. TÄƒng gradient_accumulation_steps: --gradient-accumulation-steps 8")
        print("   3. Set environment variable: set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")

    # ThÃªm callback Ä‘á»ƒ clear cache má»—i 50 steps
    trainer.add_callback(ClearCacheCallback(clear_interval=50))

    # Xá»­ lÃ½ train theo chunk náº¿u Ä‘Æ°á»£c yÃªu cáº§u
    if args.chunk_size and args.chunk_size > 0:
        total_samples = len(train_dataset)
        num_chunks = (total_samples + args.chunk_size - 1) // args.chunk_size
        start_chunk = max(0, min(args.start_chunk, num_chunks - 1))

        print(f"\n=== Train theo tá»«ng Ä‘á»£t (Chunked Training) ===")
        print(f"Tá»•ng samples: {total_samples}")
        print(f"Chunk size: {args.chunk_size} samples/chunk")
        print(f"Tá»•ng sá»‘ chunk: {num_chunks}")
        print(f"Báº¯t Ä‘áº§u tá»« chunk: {start_chunk}")
        if args.no_eval:
            print(f"âš ï¸  Evaluation Ä‘Ã£ táº¯t - Train liÃªn tá»¥c qua cÃ¡c chunks, khÃ´ng test")
        print(f"â†’ Sáº½ train liÃªn tá»¥c tá»« chunk {start_chunk} Ä‘áº¿n chunk {num_chunks - 1}")

        from pathlib import Path as _Path
        checkpoints_root = _Path(args.output_dir) / "chunk_checkpoints"
        checkpoints_root.mkdir(parents=True, exist_ok=True)

        # Train láº§n lÆ°á»£t tá»«ng chunk
        for chunk_idx in range(start_chunk, num_chunks):
            start_idx = chunk_idx * args.chunk_size
            end_idx = min(start_idx + args.chunk_size, total_samples)
            indices = list(range(start_idx, end_idx))
            chunk_dataset = train_dataset.select(indices)

            print(f"\n--- Chunk {chunk_idx + 1}/{num_chunks}: samples [{start_idx}:{end_idx}) ---")
            trainer.train_dataset = chunk_dataset

            # KhÃ´ng resume Trainer state giá»¯a cÃ¡c chunk (trÃ¡nh tráº¡ng thÃ¡i "Ä‘Ã£ hoÃ n táº¥t")
            # Model weights Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t sau má»—i chunk nÃªn tiáº¿p tá»¥c train trá»±c tiáº¿p
            trainer.train(resume_from_checkpoint=False)

            # LÆ°u checkpoint theo chunk Ä‘á»ƒ an toÃ n
            chunk_ckpt_dir = checkpoints_root / f"chunk_{chunk_idx:04d}"
            chunk_ckpt_dir.mkdir(parents=True, exist_ok=True)
            print(f"LÆ°u checkpoint chunk vÃ o: {str(chunk_ckpt_dir)}")
            trainer.save_model(str(chunk_ckpt_dir))
            trainer.save_state()
            # LÆ°u processor (tokenizer/feature extractor) má»™t láº§n á»Ÿ output_dir chÃ­nh
            try:
                processor.save_pretrained(args.output_dir)
            except Exception:
                pass

            # Clear cache giá»¯a cÃ¡c chunk
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            import gc as _gc
            _gc.collect()

        print("\nÄÃ£ hoÃ n táº¥t train theo tá»«ng Ä‘á»£t.")
    else:
        # Train thÃ´ng thÆ°á»ng trÃªn toÃ n bá»™ dataset (chá»‰ khi chunk_size = 0)
        trainer.train_dataset = train_dataset
        trainer.train()
    
    # Save model
    print(f"Äang lÆ°u model vÃ o: {args.output_dir}")
    trainer.save_model()
    processor.save_pretrained(args.output_dir)
    
    # ÄÃ¡nh giÃ¡ sau khi train vÃ  lÆ°u bÃ¡o cÃ¡o (chá»‰ náº¿u khÃ´ng táº¯t evaluation)
    if not args.no_eval:
        print("\n" + "="*70)
        print("ğŸ“Š Báº®T Äáº¦U ÄÃNH GIÃ MÃ” HÃŒNH")
        print("="*70 + "\n")
        
        results = {}
        if eval_dataset:
            try:
                print("Äang Ä‘Ã¡nh giÃ¡ trÃªn validation set...")
                eval_metrics = trainer.evaluate()
                results["eval_during_training"] = eval_metrics
                print("âœ… HoÃ n thÃ nh Ä‘Ã¡nh giÃ¡ validation set")
            except Exception as e:
                print(f"âš ï¸  Warning: KhÃ´ng thá»ƒ evaluate trÃªn eval_dataset: {e}")

        # ÄÃ¡nh giÃ¡ bá»• sung trÃªn JSONL khÃ¡c (vÃ­ dá»¥ test)
        if args.eval_after_train_jsonl:
            try:
                extra_path = os.path.abspath(args.eval_after_train_jsonl)
                if os.path.exists(extra_path):
                    print(f"\nÄang Ä‘Ã¡nh giÃ¡ bá»• sung trÃªn: {os.path.basename(extra_path)}")
                    extra_dataset = load_jsonl_dataset(extra_path, args.audio_dir, use_negative_samples=False)
                    # Táº¡o má»™t Trainer táº¡m Ä‘á»ƒ predict trÃªn extra dataset
                    extra_trainer = Seq2SeqTrainer(
                        args=training_args,
                        model=trainer.model,
                        eval_dataset=extra_dataset,
                        tokenizer=processor.feature_extractor,
                        data_collator=data_collator,
                        compute_metrics=lambda pred: compute_metrics(pred, processor),
                    )
                    extra_metrics = extra_trainer.evaluate(eval_dataset=extra_dataset)
                    results["eval_after_training"] = {
                        "path": extra_path,
                        "metrics": extra_metrics,
                    }
                    print("âœ… HoÃ n thÃ nh Ä‘Ã¡nh giÃ¡ test set")
                else:
                    print(f"âš ï¸  Warning: eval_after_train_jsonl khÃ´ng tá»“n táº¡i: {extra_path}")
            except Exception as e:
                print(f"âš ï¸  Warning: KhÃ´ng thá»ƒ evaluate bá»• sung: {e}")

        # Táº¡o vÃ  lÆ°u bÃ¡o cÃ¡o chi tiáº¿t
        if results:
            print("\n" + "="*70)
            print("ğŸ“‹ Táº O BÃO CÃO Káº¾T QUáº¢")
            print("="*70 + "\n")
            save_detailed_report(results, args.output_dir, args.model_name)
        else:
            print("âš ï¸  KhÃ´ng cÃ³ káº¿t quáº£ Ä‘Ã¡nh giÃ¡ Ä‘á»ƒ táº¡o bÃ¡o cÃ¡o")
    else:
        print("\nâš ï¸  Evaluation Ä‘Ã£ Ä‘Æ°á»£c táº¯t (--no-eval) - Bá» qua Ä‘Ã¡nh giÃ¡")

    print("\n" + "="*70)
    print("âœ… HOÃ€N THÃ€NH FINE-TUNING!")
    print("="*70)
    print(f"ğŸ“ Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {args.output_dir}")
    if not args.no_eval:
        print(f"ğŸ“Š BÃ¡o cÃ¡o Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i:")
        print(f"   - {os.path.join(args.output_dir, 'evaluation_report.txt')}")
        print(f"   - {os.path.join(args.output_dir, 'training_report.json')}")
    print("="*70)


if __name__ == '__main__':
    main()

