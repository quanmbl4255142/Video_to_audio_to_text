#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script cáº£i thiá»‡n dataset:
1. Normalize text (loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t)
2. Loáº¡i bá» hoáº·c xá»­ lÃ½ negative samples
3. Xá»­ lÃ½ cÃ¢u quÃ¡ ngáº¯n/dÃ i
"""

import json
import argparse
import re
from pathlib import Path


def normalize_text(text):
    """
    Normalize text: loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, normalize khoáº£ng tráº¯ng
    """
    if not text:
        return ""
    
    # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t
    text = text.replace('\\r\\n', ' ').replace('\\r', ' ').replace('\\n', ' ')
    text = text.replace('\\t', ' ').replace('\t', ' ')
    
    # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
    text = ' '.join(text.split())
    
    return text.strip()


def should_keep_sample(sentence, min_words=2, max_words=60):
    """
    Kiá»ƒm tra xem sample cÃ³ nÃªn Ä‘Æ°á»£c giá»¯ láº¡i khÃ´ng
    """
    if not sentence or not sentence.strip():
        return False, "CÃ¢u trá»‘ng"
    
    word_count = len(sentence.split())
    
    if word_count < min_words:
        return False, f"CÃ¢u quÃ¡ ngáº¯n ({word_count} tá»«)"
    
    if word_count > max_words:
        return False, f"CÃ¢u quÃ¡ dÃ i ({word_count} tá»«)"
    
    return True, None


def improve_dataset(input_jsonl, output_jsonl, 
                   remove_negative=False, 
                   normalize=True,
                   min_words=2,
                   max_words=60,
                   stats_only=False):
    """
    Cáº£i thiá»‡n dataset
    """
    print(f"Äang Ä‘á»c dataset tá»«: {input_jsonl}")
    
    data = []
    with open(input_jsonl, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data.append(json.loads(line))
            except json.JSONDecodeError as e:
                print(f"Warning: Bá» qua dÃ²ng khÃ´ng há»£p lá»‡: {e}")
                continue
    
    print(f"Tá»•ng sá»‘ máº«u ban Ä‘áº§u: {len(data):,}")
    
    # Thá»‘ng kÃª
    stats = {
        'total': len(data),
        'positive': 0,
        'negative': 0,
        'removed_negative': 0,
        'removed_short': 0,
        'removed_long': 0,
        'removed_empty': 0,
        'normalized': 0,
        'final': 0
    }
    
    improved_data = []
    
    for item in data:
        original_sentence = item.get('sentence', '')
        is_match = item.get('is_match', True)
        
        # Thá»‘ng kÃª
        if is_match:
            stats['positive'] += 1
        else:
            stats['negative'] += 1
        
        # Loáº¡i bá» negative samples náº¿u Ä‘Æ°á»£c yÃªu cáº§u
        if remove_negative and not is_match:
            stats['removed_negative'] += 1
            continue
        
        # Normalize text
        if normalize:
            normalized_sentence = normalize_text(original_sentence)
            if normalized_sentence != original_sentence:
                stats['normalized'] += 1
            sentence = normalized_sentence
        else:
            sentence = original_sentence
        
        # Kiá»ƒm tra Ä‘á»™ dÃ i
        keep, reason = should_keep_sample(sentence, min_words, max_words)
        if not keep:
            if reason.startswith("CÃ¢u trá»‘ng"):
                stats['removed_empty'] += 1
            elif reason.startswith("CÃ¢u quÃ¡ ngáº¯n"):
                stats['removed_short'] += 1
            elif reason.startswith("CÃ¢u quÃ¡ dÃ i"):
                stats['removed_long'] += 1
            continue
        
        # Táº¡o item má»›i vá»›i text Ä‘Ã£ normalize
        new_item = item.copy()
        new_item['sentence'] = sentence
        improved_data.append(new_item)
    
    stats['final'] = len(improved_data)
    
    # In thá»‘ng kÃª
    print(f"\n{'='*70}")
    print(f"ğŸ“Š THá»NG KÃŠ Cáº¢I THIá»†N DATASET")
    print(f"{'='*70}")
    print(f"Tá»•ng sá»‘ máº«u ban Ä‘áº§u: {stats['total']:,}")
    print(f"  - Positive: {stats['positive']:,}")
    print(f"  - Negative: {stats['negative']:,}")
    print(f"\nÄÃ£ loáº¡i bá»:")
    print(f"  - Negative samples: {stats['removed_negative']:,}")
    print(f"  - CÃ¢u trá»‘ng: {stats['removed_empty']:,}")
    print(f"  - CÃ¢u quÃ¡ ngáº¯n (<{min_words} tá»«): {stats['removed_short']:,}")
    print(f"  - CÃ¢u quÃ¡ dÃ i (>{max_words} tá»«): {stats['removed_long']:,}")
    print(f"\nÄÃ£ normalize: {stats['normalized']:,} cÃ¢u")
    print(f"\nâœ… Sá»‘ máº«u cuá»‘i cÃ¹ng: {stats['final']:,} ({stats['final']/stats['total']*100:.1f}%)")
    print(f"{'='*70}\n")
    
    if stats_only:
        return stats
    
    # LÆ°u dataset Ä‘Ã£ cáº£i thiá»‡n
    if output_jsonl:
        print(f"Äang lÆ°u dataset Ä‘Ã£ cáº£i thiá»‡n vÃ o: {output_jsonl}")
        output_path = Path(output_jsonl)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_jsonl, 'w', encoding='utf-8') as f:
            for item in improved_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"âœ… ÄÃ£ lÆ°u {len(improved_data):,} máº«u vÃ o {output_jsonl}")
    
    return stats


def main():
    parser = argparse.ArgumentParser(
        description='Cáº£i thiá»‡n dataset: normalize text, loáº¡i bá» negative samples, xá»­ lÃ½ cÃ¢u quÃ¡ ngáº¯n/dÃ i'
    )
    parser.add_argument('--input', type=str, required=True,
                       help='File JSONL input (vÃ­ dá»¥: data/train.jsonl)')
    parser.add_argument('--output', type=str, default=None,
                       help='File JSONL output (vÃ­ dá»¥: data/train_improved.jsonl). Náº¿u khÃ´ng chá»‰ Ä‘á»‹nh, sáº½ in thá»‘ng kÃª mÃ  khÃ´ng lÆ°u')
    parser.add_argument('--remove-negative', action='store_true',
                       help='Loáº¡i bá» negative samples (is_match=False)')
    parser.add_argument('--no-normalize', dest='normalize', action='store_false', default=True,
                       help='Táº¯t normalize text')
    parser.add_argument('--min-words', type=int, default=2,
                       help='Sá»‘ tá»« tá»‘i thiá»ƒu (máº·c Ä‘á»‹nh: 2)')
    parser.add_argument('--max-words', type=int, default=60,
                       help='Sá»‘ tá»« tá»‘i Ä‘a (máº·c Ä‘á»‹nh: 60)')
    parser.add_argument('--stats-only', action='store_true',
                       help='Chá»‰ in thá»‘ng kÃª, khÃ´ng lÆ°u file')
    
    args = parser.parse_args()
    
    if not Path(args.input).exists():
        print(f"âŒ File khÃ´ng tá»“n táº¡i: {args.input}")
        return
    
    improve_dataset(
        input_jsonl=args.input,
        output_jsonl=None if args.stats_only else (args.output or args.input.replace('.jsonl', '_improved.jsonl')),
        remove_negative=args.remove_negative,
        normalize=args.normalize,
        min_words=args.min_words,
        max_words=args.max_words,
        stats_only=args.stats_only
    )


if __name__ == "__main__":
    main()

